<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(5)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(5) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(5) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(5) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Homework 4 | COMS4732W Computer Vision 2</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="Homework 4" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="A starter template for a Jeykll site using the Just the Docs theme!" /> <meta property="og:description" content="A starter template for a Jeykll site using the Just the Docs theme!" /> <link rel="canonical" href="http://localhost:4000/hw4/" /> <meta property="og:url" content="http://localhost:4000/hw4/" /> <meta property="og:site_name" content="COMS4732W Computer Vision 2" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Homework 4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"A starter template for a Jeykll site using the Just the Docs theme!","headline":"Homework 4","url":"http://localhost:4000/hw4/"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <header class="side-bar"> <div class="site-header"> <a href="/" class="site-title lh-tight"> COMS4732W Computer Vision 2 </a> <button id="menu-button" class="site-button btn-reset" aria-label="Menu" aria-expanded="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="/hw1/" class="nav-list-link">Homework 1</a></li><li class="nav-list-item"><a href="/hw2/" class="nav-list-link">Homework 2</a></li><li class="nav-list-item"><a href="/hw3/" class="nav-list-link">Homework 3</a></li><li class="nav-list-item"><a href="/hw4/" class="nav-list-link">Homework 4</a></li><li class="nav-list-item"><a href="/hw5/" class="nav-list-link">Homework 5</a></li></ul> </nav> <div class="d-md-block d-xs-none"> <div class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </div> </div> </header> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search COMS4732W Computer Vision 2" autocomplete="off"> <label for="search-input" class="search-label"> <span class="sr-only">Search COMS4732W Computer Vision 2</span> <svg viewBox="0 0 24 24" class="search-icon" aria-hidden="true"><use xlink:href="#svg-search"></use></svg> </label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/just-the-docs/just-the-docs-template" class="site-button" > Template Repository </a> </li> </ul> </nav> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] } }; </script> <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <h1 id="homework-4"> <a href="#homework-4" class="anchor-heading" aria-labelledby="homework-4"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Homework 4 </h1> <p><a href="https://cal-cs180.github.io/fa25/">CS180: Intro to Computer Vision and Computational Photography</a></p> <h1 id="neural-radiance-field"> <a href="#neural-radiance-field" class="anchor-heading" aria-labelledby="neural-radiance-field"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Neural Radiance Field! </h1> <h2 id="due-date-november-14th-2025"> <a href="#due-date-november-14th-2025" class="anchor-heading" aria-labelledby="due-date-november-14th-2025"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Due Date: November 14th, 2025 </h2> <p><strong>Note on compute requirements:</strong> We’re using PyTorch to implement neural networks with GPU acceleration. If you have an M-series Mac (M1/M2/M3), you should be able to run everything locally using the <a href="https://pytorch.org/docs/stable/notes/mps.html">MPS backend</a>. For older or less powerful hardware, we recommend using GPUs from <a href="https://colab.research.google.com/">Colab</a>. Colab Pro is now <a href="https://colab.research.google.com/signup">free</a> for students.</p><hr /> <h1 id="part-0-calibrating-your-camera-and-capturing-a-3d-scan"> <a href="#part-0-calibrating-your-camera-and-capturing-a-3d-scan" class="anchor-heading" aria-labelledby="part-0-calibrating-your-camera-and-capturing-a-3d-scan"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 0: Calibrating Your Camera and Capturing a 3D Scan </h1> <p>For the first part of the assignment, you will take a 3D scan of your own object which you will build a NeRF of later! To do this, we will use visual tracking targets called <a href="https://cs-courses.mines.edu/csci507/schedule/24/ArUco.pdf">ArUco tags</a>, which provide a way to reliably detect the same 3D keypoints across different images. There are 2 parts: 1) calibrating your camera parameters, and 2) using them to estimate pose. This part can be done entirely locally on your laptop, no need for a GPU. We will be using many helper functions from <a href="https://opencv.org/">OpenCV</a>, which you can install with <code class="language-plaintext highlighter-rouge">pip install opencv-python</code>.</p><hr /> <h2 id="part-01-calibrating-your-camera"> <a href="#part-01-calibrating-your-camera" class="anchor-heading" aria-labelledby="part-01-calibrating-your-camera"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 0.1: Calibrating Your Camera </h2> <p>First, either print out the following <a href="/hws/hw4/assets/aruco_grid-2.pdf">calibration tags</a> or pull them up on a laptop/iPad screen. Capture 30-50 images of these tags from your phone camera, <strong>keeping the zoom the same</strong>. While capturing, results will be best if you vary the angle/distance of the camera, like shown in chessboard calibration in lecture.</p> <p><strong>Note:</strong> For the purpose of this assignment, phone cameras work better since they provide undistorted images without lens effects. If you use a real camera, try to make sure there are no distortion effects.</p> <p>Now you’ll write a script to calibrate your camera using the images you captured. Here’s the pipeline:</p> <ol> <li>Loop through all your calibration images</li> <li>For each image, detect the ArUco tags using OpenCV’s ArUco detector</li> <li>Extract the corner coordinates from the detected tags</li> <li>Collect all detected corners and their corresponding 3D world coordinates (you can consider the ArUco tag as the world origin and define the 4 corners’ 3D points relative to that, e.g., if your tag is 0.02m × 0.02m, the corners could be [(0,0,0), (0.02,0,0), (0.02,0.02,0), (0,0.02,0)])</li> <li>Use <a href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga3207604e4b1a1758aa66acb6ed5aa65d"><code class="language-plaintext highlighter-rouge">cv2.calibrateCamera()</code></a> to compute the camera intrinsics and distortion coefficients</li> </ol> <p><strong>Important:</strong> Your code should handle cases where tags aren’t detected in some images (this is common). Make sure to check if tags were found before trying to use the detection results, otherwise your script will crash.</p> <p>Here’s a code snippet to get you started with detecting ArUco tags (the tags in the PDF are 4x4):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Create ArUco dictionary and detector parameters (4x4 tags)
</span><span class="n">aruco_dict</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">aruco</span><span class="p">.</span><span class="nf">getPredefinedDictionary</span><span class="p">(</span><span class="n">cv2</span><span class="p">.</span><span class="n">aruco</span><span class="p">.</span><span class="n">DICT_4X4_50</span><span class="p">)</span>
<span class="n">aruco_params</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">aruco</span><span class="p">.</span><span class="nc">DetectorParameters</span><span class="p">()</span>

<span class="c1"># Detect ArUco markers in an image
# Returns: corners (list of numpy arrays), ids (numpy array)
</span><span class="n">corners</span><span class="p">,</span> <span class="n">ids</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">aruco</span><span class="p">.</span><span class="nf">detectMarkers</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">aruco_dict</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="n">aruco_params</span><span class="p">)</span>

<span class="c1"># Check if any markers were detected
</span><span class="k">if</span> <span class="n">ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Process the detected corners
</span>    <span class="c1"># corners: list of length N (number of detected tags)
</span>    <span class="c1">#   - each element is a numpy array of shape (1, 4, 2) containing the 4 corner coordinates (x, y)
</span>    <span class="c1"># ids: numpy array of shape (N, 1) containing the tag IDs for each detected marker
</span>    <span class="c1"># Example: if 3 tags detected, corners will be a list of 3 arrays, ids will be shape (3, 1)
</span><span class="k">else</span><span class="p">:</span>
    <span class="c1"># No tags detected in this image, skip it
</span>    <span class="k">pass</span>
</code></pre></div></div><hr /> <h2 id="part-02-capturing-a-3d-object-scan"> <a href="#part-02-capturing-a-3d-object-scan" class="anchor-heading" aria-labelledby="part-02-capturing-a-3d-object-scan"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 0.2: Capturing a 3D Object Scan </h2> <p>Next, pick a favorite object of yours, and print out a <strong>single</strong> ArUco tag, which you can generate from <a href="https://chev.me/arucogen/">here</a>. Place the object next to the tag on a tabletop, and capture 30-50 images of the object from different angles. <strong>Important: Use the same camera and zoom level as you did for calibration.</strong> If you cut out the ArUco tag from a piece of paper, make sure to leave a white border around it, otherwise detection will fail.</p> <p>Capture tips for good quality NeRF results later:</p> <ol> <li>Avoid brightness/exposure changes (this is why printing the tag will work better than using a tablet display)</li> <li>Avoid blurry images, try not to introduce motion blur</li> <li>Capture images at varying angles horizontally and vertically</li> <li>Capture images at one uniform distance, about 10-20cm away from the object so that it fills ~50% of the frame</li> </ol><hr /> <h2 id="part-03-estimating-camera-pose"> <a href="#part-03-estimating-camera-pose" class="anchor-heading" aria-labelledby="part-03-estimating-camera-pose"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 0.3: Estimating Camera Pose </h2> <p>Now that you have calibrated your camera, you can use those intrinsic parameters to estimate the camera pose (position and orientation) for each image of your object. As discussed in lecture, this is the classic <strong>Perspective-n-Point (PnP)</strong> problem: given a set of 3D points in world coordinates and their corresponding 2D projections in an image, we want to find the camera’s extrinsic parameters (rotation and translation).</p> <p>For each image in your object scan, you’ll detect the single ArUco tag and use <code class="language-plaintext highlighter-rouge">cv2.solvePnP()</code> to estimate the camera pose. Here’s what you need:</p> <p><strong>Inputs to solvePnP:</strong></p> <ul> <li><strong>objectPoints</strong> (numpy array of shape (N, 3) or (N, 1, 3)): The 3D coordinates of the ArUco tag corners in world space. Since you know the physical size of your printed tag, you can define these coordinates. For example, if your tag is 0.02m × 0.02m, you might define the 4 corners as: [(0,0,0), (0.02,0,0), (0.02,0.02,0), (0,0.02,0)] in meters, with the tag lying flat on the z=0 plane.</li> <li><strong>imagePoints</strong> (numpy array of shape (N, 2) or (N, 1, 2)): The detected 2D pixel coordinates of the tag corners in the image (from <code class="language-plaintext highlighter-rouge">detectMarkers()</code>). You’ll need to reshape the corners array from detectMarkers to match this shape.</li> <li><strong>cameraMatrix</strong> (numpy array of shape (3, 3)): The intrinsic matrix K that you computed in Part 0.1 (contains focal length and principal point)</li> <li><strong>distCoeffs</strong> (numpy array or None): The distortion coefficients from calibration</li> </ul> <p><strong>Output from solvePnP:</strong></p> <ul> <li><strong>success</strong> (bool): Whether the pose estimation succeeded</li> <li><strong>rvec</strong> (numpy array of shape (3, 1)): Axis-Angle rotation vector (can be converted to a 3×3 rotation matrix using <code class="language-plaintext highlighter-rouge">cv2.Rodrigues()</code>)</li> <li><strong>tvec</strong> (numpy array of shape (3, 1)): Translation vector</li> </ul> <p>Together, the rotation matrix R (from rvec) and translation vector tvec form the camera’s extrinsic matrix, which describes where the camera is positioned and oriented relative to the ArUco tag’s coordinate system (which we’re treating as the world origin). <strong>Note:</strong> OpenCV’s <code class="language-plaintext highlighter-rouge">solvePnP()</code> returns the world-to-camera transformation; you will need to invert this to get the camera-to-world (c2w) matrix for visualization and Part 0.4.</p> <p><strong>Important:</strong> Just like in Part 0.1, make sure your code handles cases where the tag isn’t detected in some images. You should skip those images rather than letting your code crash.</p> <p>To help visualize your pose estimation results, you can use the following code snippet to display a camera frustum in 3D (<code class="language-plaintext highlighter-rouge">pip install viser</code>):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">viser</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">server</span> <span class="o">=</span> <span class="n">viser</span><span class="p">.</span><span class="nc">ViserServer</span><span class="p">(</span><span class="n">share</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># Example of visualizing a camera frustum (in practice loop over all images)
# c2w is the camera-to-world transformation matrix (3x4), and K is the camera intrinsic matrix (3x3)
</span><span class="n">server</span><span class="p">.</span><span class="n">scene</span><span class="p">.</span><span class="nf">add_camera_frustum</span><span class="p">(</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">/cameras/</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># give it a name
</span>    <span class="n">fov</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="n">H</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="c1"># field of view
</span>    <span class="n">aspect</span><span class="o">=</span><span class="n">W</span> <span class="o">/</span> <span class="n">H</span><span class="p">,</span> <span class="c1"># aspect ratio
</span>    <span class="n">scale</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="c1"># scale of the camera frustum change if too small/big
</span>    <span class="n">wxyz</span><span class="o">=</span><span class="n">viser</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="n">SO3</span><span class="p">.</span><span class="nf">from_matrix</span><span class="p">(</span><span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]).</span><span class="n">wxyz</span><span class="p">,</span> <span class="c1"># orientation in quaternion format
</span>    <span class="n">position</span><span class="o">=</span><span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="c1"># position of the camera
</span>    <span class="n">image</span><span class="o">=</span><span class="n">img</span> <span class="c1"># image to visualize
</span><span class="p">)</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Wait to allow visualization to run
</span></code></pre></div></div> <p><strong>[Deliverables]</strong> Include 2 screenshots of your cloud of cameras in Viser showing the camera frustums’ poses and images.</p><hr /> <h2 id="part-04-undistorting-images-and-creating-a-dataset"> <a href="#part-04-undistorting-images-and-creating-a-dataset" class="anchor-heading" aria-labelledby="part-04-undistorting-images-and-creating-a-dataset"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 0.4: Undistorting images and creating a dataset </h2> <p>Now that you have the camera intrinsics and pose estimates, the final step is to undistort your images and package everything into a dataset format that you can use for training NeRF in the later parts of this project.</p> <p>First, use <code class="language-plaintext highlighter-rouge">cv2.undistort()</code> to remove any lens distortion from your images. This is important because NeRF assumes a perfect pinhole camera model without distortion.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cv2</span>

<span class="c1"># Undistort an image using the calibration results
</span><span class="n">undistorted_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">undistort</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">camera_matrix</span><span class="p">,</span> <span class="n">dist_coeffs</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Note on black boundaries:</strong> If you see black boundaries after undistorting your images, you can use <code class="language-plaintext highlighter-rouge">cv2.getOptimalNewCameraMatrix()</code> to compute a new camera matrix that crops out the invalid pixels. This function returns both a new intrinsics matrix and a valid pixel ROI (region of interest). You can then crop your undistorted images to this ROI to remove the black borders. <strong>Important:</strong> If you crop your images this way, make sure to update the principal point in your intrinsics matrix to account for the crop offset, as the coordinate system origin has shifted.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cv2</span>

<span class="c1"># Example: Handling black boundaries from undistortion
</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span class="c1"># alpha=1 keeps all pixels (more black borders), alpha=0 crops maximally
</span><span class="n">new_camera_matrix</span><span class="p">,</span> <span class="n">roi</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">getOptimalNewCameraMatrix</span><span class="p">(</span>
    <span class="n">camera_matrix</span><span class="p">,</span> <span class="n">dist_coeffs</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">undistorted_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">undistort</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">camera_matrix</span><span class="p">,</span> <span class="n">dist_coeffs</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">new_camera_matrix</span><span class="p">)</span>

<span class="c1"># Crop to the valid region of interest
</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w_roi</span><span class="p">,</span> <span class="n">h_roi</span> <span class="o">=</span> <span class="n">roi</span>
<span class="n">undistorted_img</span> <span class="o">=</span> <span class="n">undistorted_img</span><span class="p">[</span><span class="n">y</span><span class="p">:</span><span class="n">y</span><span class="o">+</span><span class="n">h_roi</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="o">+</span><span class="n">w_roi</span><span class="p">]</span>

<span class="c1"># Update the principal point to account for the crop offset
</span><span class="n">new_camera_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-=</span> <span class="n">x</span>  <span class="c1"># cx
</span><span class="n">new_camera_matrix</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-=</span> <span class="n">y</span>  <span class="c1"># cy
</span>
<span class="c1"># Use new_camera_matrix (with updated principal point) when creating your dataset!
</span></code></pre></div></div> <p>After undistorting all your images, you’ll need to save everything in a <code class="language-plaintext highlighter-rouge">.npz</code> file format that matches what’s expected for the NeRF training code. You should split your images into training, validation, and test sets, then save using the following keys:</p> <ul> <li><strong>images_train</strong>: numpy array of shape (N_train, H, W, 3) containing your undistorted training images (0-255 range, will be normalized when loaded)</li> <li><strong>c2ws_train</strong>: numpy array of shape (N_train, 4, 4) containing the camera-to-world transformation matrices for training images</li> <li><strong>images_val</strong>: numpy array of shape (N_val, H, W, 3) for validation images</li> <li><strong>c2ws_val</strong>: numpy array of shape (N_val, 4, 4) for validation camera poses</li> <li><strong>c2ws_test</strong>: numpy array of shape (N_test, 4, 4) for test camera poses (used for novel view rendering)</li> <li><strong>focal</strong>: float representing the focal length from your camera intrinsics (assuming fx = fy)</li> </ul> <p>You can save your dataset using <code class="language-plaintext highlighter-rouge">np.savez()</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Package your data (keep images in 0-255 range, they'll be normalized when loaded)
</span><span class="n">np</span><span class="p">.</span><span class="nf">savez</span><span class="p">(</span>
    <span class="sh">'</span><span class="s">my_data.npz</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">images_train</span><span class="o">=</span><span class="n">images_train</span><span class="p">,</span>    <span class="c1"># (N_train, H, W, 3)
</span>    <span class="n">c2ws_train</span><span class="o">=</span><span class="n">c2ws_train</span><span class="p">,</span>        <span class="c1"># (N_train, 4, 4)
</span>    <span class="n">images_val</span><span class="o">=</span><span class="n">images_val</span><span class="p">,</span>        <span class="c1"># (N_val, H, W, 3)
</span>    <span class="n">c2ws_val</span><span class="o">=</span><span class="n">c2ws_val</span><span class="p">,</span>            <span class="c1"># (N_val, 4, 4)
</span>    <span class="n">c2ws_test</span><span class="o">=</span><span class="n">c2ws_test</span><span class="p">,</span>          <span class="c1"># (N_test, 4, 4)
</span>    <span class="n">focal</span><span class="o">=</span><span class="n">focal</span>                   <span class="c1"># float
</span><span class="p">)</span>
</code></pre></div></div> <p>This dataset can then be loaded in Parts 1 and 2 the same way the provided lego dataset is loaded, allowing you to train a NeRF on your own captured object!</p> <p>As a sanity check you can test your calibration implementation on our <a href="https://drive.google.com/drive/folders/1utiPjbEvnf87Eg4h-7WYVTXk2q6lI4rn?usp=sharing">calibration images</a> and our <a href="https://drive.google.com/drive/folders/1aoLM-ay2ZjatVIEtFW7QtuNyjciwydsC?usp=sharing">Lafufu dataset</a> which we used to train the NeRF example seen at the end of this project.</p><hr /> <h1 id="part-1-fit-a-neural-field-to-a-2d-image"> <a href="#part-1-fit-a-neural-field-to-a-2d-image" class="anchor-heading" aria-labelledby="part-1-fit-a-neural-field-to-a-2d-image"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 1: Fit a Neural Field to a 2D Image </h1> <p>From the lecture we know that we can use a Neural Radiance Field (NeRF) (\(F: \{x, y, z, \theta, \phi\} \rightarrow \{r, g, b, \sigma\}\)) to represent a 3D space. But before jumping into 3D, let’s first get familar with NeRF (and PyTorch) using a 2D example. In fact, since there is no concept of radiance in 2D, the Neural Radiance Field falls back to just a Neural Field (\(F: \{u, v\} \rightarrow \{r, g, b\}\)) in 2D, in which \(\{u, v\}\) is the pixel coordinate. In this section, we will create a neural field that can represent a 2D image and optimize that neural field to fit this image. You can start from <a href="https://live.staticflickr.com/7492/15677707699_d9d67acf9d_b.jpg">this image</a>, but feel free to try out any other images.</p> <p><strong>[Impl: Network]</strong> You would need to create an <em>Multilayer Perceptron (MLP)</em> network with <em>Sinusoidal Positional Encoding (PE)</em> that takes in the 2-dim pixel coordinates, and output the 3-dim pixel colors.</p> <ul> <li> <p>Multilayer Perceptron (MLP): An MLP is simply a stack of non linear activations (e.g., <a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html">torch.nn.ReLU()</a> or <a href="https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html">torch.nn.Sigmoid()</a>) and fully connected layers (<a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">torch.nn.Linear()</a>). For this part, you can start from building an MLP with the structure shown in the image below. Note that you would need to have a Sigmoid layer at the end of the MLP to constrain the network output be in the range of (0, 1), as a valid pixel color (don’t forget to also normalize your image from [0, 255] to [0, 1] when you use it for supervision!). You can take a reference from <a href="https://machinelearningmastery.com/building-multilayer-perceptron-models-in-pytorch/">this tutorial</a> on how to create an MLP in PyTorch.</p> <p><img src="/hws/hw4/assets/mlp_img.jpg" alt="MLP" /></p> </li> <li> <p>Sinusoidal Positional Encoding (PE): PE is an operation that you apply a series of sinusoidal functions to the input coordinates, to expand its dimensionality (See equation 4 from <a href="https://arxiv.org/pdf/2003.08934.pdf">this paper</a> for reference). Note we also additionally keep the original input in PE, so the complete formulation is \(PE(x) = \{x, \sin(2^0\pi x), \cos(2^0\pi x), \sin(2^1\pi x), \cos(2^1\pi x), ..., \sin(2^{L-1}\pi x), \cos(2^{L-1}\pi x)\}\) in which \(L\) is the highest frequency level. You can start from \(L=10\) that maps a 2 dimension coordinate to a 42 dimension vector.</p> </li> </ul> <p><strong>[Impl: Dataloader]</strong> If the image is with high resolution, it might be not feasible train the network with the all the pixels in every iteration due to the GPU memory limit. So you need to implement a dataloader that randomly sample \(N\) pixels at every iteration for training. The dataloader is expected to return both the \(N\times2\) 2D coordinates and \(N\times3\) colors of the pixels, which will serve as the input to your network, and the supervision target, respectively (essentially you have a batch size of \(N\)). You would want to normalize both the coordinates (x = x / image_width, y = y / image_height) and the colors (rgbs = rgbs / 255.0) to make them within the range of [0, 1].</p> <p><strong>[Impl: Loss Function, Optimizer, and Metric]</strong> Now that you have the network (MLP) and the dataloader, you need to define the loss function and the optimizer before you can start training your network. You will use mean squared error loss (MSE) (<a href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html">torch.nn.MSELoss</a>) between the predicted color and the groundtruth color. Train your network using Adam (<a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">torch.optim.Adam</a>) with a learning rate of 1e-2. Run the training loop for 1000 to 3000 iterations with a batch size of 10k. For the metric, MSE is a good one but it is more common to use <a href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">Peak signal-to-noise ratio (PSNR)</a> when it comes to measuring the reconstruction quality of a image. If the image is normalized to [0, 1], you can use the following equation to compute PSNR from MSE: \(\text{PSNR} = 10 \cdot \log_{10}\left(\frac{1}{\text{MSE}}\right)\)</p> <p><strong>[Impl: Hyperparameter Tuning]</strong> Vary the layer width (channel size) and the max frequency \(L\) for the positional encoding.</p> <p><img src="/hws/hw4/assets/2D_training.jpg" alt="training" /></p> <p><strong>[Deliverables]</strong> As a reference, the above images show the process of optimizing the network to fit on this image.</p> <ul> <li>Report your model architecture including number of layers, width, and learning rate. Feel free to add other details you think are important.</li> <li>Show training progression (images at different iterations, similar to the above reference) on both the provided test image and one of your own images.</li> <li>Show final results for 2 choices of max positional encoding frequency and 2 choices of width (a 2x2 grid of results). Try very low values for these hyperparameters to see how it affects the outputs.</li> <li>Show the PSNR curve for training on one image of your choice.</li> </ul><hr /> <h1 id="part-2-fit-a-neural-radiance-field-from-multi-view-images"> <a href="#part-2-fit-a-neural-radiance-field-from-multi-view-images" class="anchor-heading" aria-labelledby="part-2-fit-a-neural-radiance-field-from-multi-view-images"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 2: Fit a Neural Radiance Field from Multi-view Images </h1> <p>Now that we are familiar with using a neural field to represent a image, we can proceed to a more interesting task that using a neural <em>radiance</em> field to represent a 3D space, through inverse rendering from multi-view calibrated images. For this part we are going to use the Lego scene from the original <a href="https://www.matthewtancik.com/nerf">NeRF paper</a>, but with lower resolution images (200 x 200) and preprocessed cameras (downloaded from <a href="/hws/hw4/assets/lego_200x200.npz">here</a>). The following code can be used to parse the data. The figure on its right shows a plot of all the cameras, including training cameras in black, validation cameras in red, and test cameras in green.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">lego_200x200.npz</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Training images: [100, 200, 200, 3]
</span><span class="n">images_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">images_train</span><span class="sh">"</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="c1"># Cameras for the training images 
# (camera-to-world transformation matrix): [100, 4, 4]
</span><span class="n">c2ws_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">c2ws_train</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Validation images: 
</span><span class="n">images_val</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">images_val</span><span class="sh">"</span><span class="p">]</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="c1"># Cameras for the validation images: [10, 4, 4]
# (camera-to-world transformation matrix): [10, 200, 200, 3]
</span><span class="n">c2ws_val</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">c2ws_val</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Test cameras for novel-view video rendering: 
# (camera-to-world transformation matrix): [60, 4, 4]
</span><span class="n">c2ws_test</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">c2ws_test</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Camera focal length
</span><span class="n">focal</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">focal</span><span class="sh">"</span><span class="p">]</span>  <span class="c1"># float
</span></code></pre></div></div> <p><img src="/hws/hw4/assets/data_plot.png" alt="data plot" /></p><hr /> <h2 id="part-21-create-rays-from-cameras"> <a href="#part-21-create-rays-from-cameras" class="anchor-heading" aria-labelledby="part-21-create-rays-from-cameras"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 2.1: Create Rays from Cameras </h2> <p><strong>Camera to World Coordinate Conversion.</strong> The transformation between the world space \(\mathbf{X_w} = (x_w, y_w, z_w)\) and the camera space \(\mathbf{X_c} = (x_c, y_c, z_c)\) can be defined as a rotation matrix \(\mathbf{R}_{3 \times 3}\) and a translation vector \(\mathbf{t}\): \(\begin{bmatrix} x_c \\ y_c \\ z_c \\ 1 \end{bmatrix} = \begin{bmatrix} \mathbf{R}_{3\times3} &amp; \mathbf{t} \\ \mathbf{0}_{1\times3} &amp; 1 \end{bmatrix} \begin{bmatrix} x_w \\ y_w \\ z_w \\ 1 \end{bmatrix}\) in which \(\begin{bmatrix} \mathbf{R}_{3\times3} &amp; \mathbf{t} \\ \mathbf{0}_{1\times3} &amp; 1 \end{bmatrix}\) is called world-to-camera (<code class="language-plaintext highlighter-rouge">w2c</code>) transformation matrix, or extrinsic matrix. The inverse of it is called camera-to-world (<code class="language-plaintext highlighter-rouge">c2w</code>) transformation matrix.</p> <p><strong>[Impl]</strong> In this session you would need to implement a function <code class="language-plaintext highlighter-rouge">x_w = transform(c2w, x_c)</code> that transform a point from camera to the world space. You can verify your implementation by checking if the follow statement is always true: <code class="language-plaintext highlighter-rouge">x == transform(c2w.inv(), transform(c2w, x))</code>. Note you might want your implementation to support batched coordinates for later use. You can implement it with either numpy or torch.</p> <p><strong>Pixel to Camera Coordinate Conversion.</strong> Consider a pinhole camera with focal length \((f_x, f_y)\) and principal point \((o_x = \text{image_width} / 2, o_y = \text{image_height} / 2)\), its intrinsic matrix \(\mathbf{K}\) is defined as: \(\mathbf{K} = \begin{bmatrix} f_x &amp; 0 &amp; o_x \\ 0 &amp; f_y &amp; o_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) which can be used to project a 3D point \((x_c, y_c, z_c)\) in the <em>camera coordinate system</em> to a 2D location \((u, v)\) in <em>pixel coordinate system</em> : \(s \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \mathbf{K} \begin{bmatrix} x_c \\ y_c \\ z_c \end{bmatrix}\) in which \(s=z_c\) is the depth of this point along the optical axis.</p> <p><strong>[Impl]</strong> In this session you would need to implement a function that invert the aforementioned process, which transform a point from the pixel coordinate system back to the camera coordinate system: <code class="language-plaintext highlighter-rouge">x_c = pixel_to_camera(K, uv, s)</code>. Similar to the previous session, you might also want your implementation here to support batched coordinates for later use. You can implement it with either numpy or torch.</p> <p><strong>Pixel to Ray.</strong> A ray can be defined by an origin vector \(\mathbf{r}_o \in \mathbb{R}^3\) and a direction vector \(\mathbf{r}_d \in \mathbb{R}^3\). In the case of a pinhole camera, we want to know the \(\{\mathbf{r}_o, \mathbf{r}_d\}\) for every pixel \((u, v)\). The origin \(\mathbf{r}_o\) of those rays is easy to get because it is just the location of the camera in world coordinates. For a camera-to-world (c2w) transformation matrix \(\begin{bmatrix} \mathbf{R}_{3\times3} &amp; \mathbf{t} \\ \mathbf{0}_{1\times3} &amp; 1 \end{bmatrix}\), the camera origin is simply the translation component: \(\mathbf{r}_o = \mathbf{t}\) To calculate the ray direction for pixel \((u, v)\), we can simply choose a point along this ray with depth equal to 1 (\(s=1\)) and find its coordinate in world space \(\mathbf{X_w} = (x_w, y_w, z_w)\) using your previously implemented functions. Then the normalized ray direction can be computed by: \(\mathbf{r}_d = \frac{\mathbf{X_w} - \mathbf{r}_o}{\|\mathbf{X_w} - \mathbf{r}_o\|_2}\)</p> <p><strong>[Impl]</strong> In this section you will need to implement a function that converts a pixel coordinate to a ray with origin and normalized direction: <code class="language-plaintext highlighter-rouge">ray_o, ray_d = pixel_to_ray(K, c2w, uv)</code>. You might find your previously implemented functions useful here. Similarly, you might also want your implementation to support batched coordinates.</p><hr /> <h2 id="part-22-sampling"> <a href="#part-22-sampling" class="anchor-heading" aria-labelledby="part-22-sampling"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 2.2: Sampling </h2> <p><strong>[Impl: Sampling Rays from Images]</strong> In Part 1, we have done random sampling on a single image to get the pixel color and pixel coordinates. Here we can build on top of that, and with the camera intrinsics &amp; extrinsics, we would be able to convert the pixel coordinates into ray origins and directions. Make sure to account for the offset from image coordinate to pixel center (this can be done simply by adding .5 to your UV pixel coordinate grid)! Since we have multiple images now, we have two options of sampling rays. Say we want to sample N rays at every training iteration, option 1 is to first sample M images, and then sample N // M rays from every image. The other option is to flatten all pixels from all images and do a global sampling once to get N rays from all images. You can choose whichever way you do ray sampling.</p> <p><strong>[Impl: Sampling Points along Rays.]</strong> After having rays, we also need to discretize each ray into samples that live in the 3D space. The simplest way is to uniformly create some samples along the ray (<code class="language-plaintext highlighter-rouge">t = np.linspace(near, far, n_samples)</code>). For the lego scene that we have, we can set <code class="language-plaintext highlighter-rouge">near=2.0</code> and <code class="language-plaintext highlighter-rouge">far=6.0</code>. The actual 3D coordinates can be acquired by $\mathbf{x} = \mathbf{r}_o + t \mathbf{r}_d$. However this would lead to a fixed set of 3D points, which could potentially lead to overfitting when we train the NeRF later on. On top of this, we want to introduce some small perturbation to the points <em>only during training</em>, so that every location along the ray would be touched upon during training. this can be achieved by something like <code class="language-plaintext highlighter-rouge">t = t + (np.random.rand(t.shape) * t_width)</code> where t is set to be the start of each interval. We recommend to set <code class="language-plaintext highlighter-rouge">n_samples</code> to 32 or 64 in this project.</p><hr /> <h2 id="part-23-putting-the-dataloading-all-together"> <a href="#part-23-putting-the-dataloading-all-together" class="anchor-heading" aria-labelledby="part-23-putting-the-dataloading-all-together"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 2.3: Putting the Dataloading All Together </h2> <p>Similar to Part 1, you would need to write a dataloader that randomly sample pixels from multiview images. What is different with Part 1, is that now you need to convert the pixel coordinates into rays in your dataloader, and return ray origin, ray direction and pixel colors from your dataloader. To verify if you have by far implement everything correctly, we here provide some visualization code to plot the cameras, rays, and samples in 3D. We additionally recommend you try this code with rays sampled only from one camera so you can make sure that all the rays stay within the camera frustum and eliminating the possibility of other smaller harder to catch bugs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">viser</span><span class="p">,</span> <span class="n">time</span>  <span class="c1"># pip install viser
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># --- You Need to Implement These ------
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nc">RaysData</span><span class="p">(</span><span class="n">images_train</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">c2ws_train</span><span class="p">)</span>
<span class="n">rays_o</span><span class="p">,</span> <span class="n">rays_d</span><span class="p">,</span> <span class="n">pixels</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">sample_rays</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># Should expect (B, 3)
</span><span class="n">points</span> <span class="o">=</span> <span class="nf">sample_along_rays</span><span class="p">(</span><span class="n">rays_o</span><span class="p">,</span> <span class="n">rays_d</span><span class="p">,</span> <span class="n">perturb</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">images_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="c1"># ---------------------------------------
</span>
<span class="n">server</span> <span class="o">=</span> <span class="n">viser</span><span class="p">.</span><span class="nc">ViserServer</span><span class="p">(</span><span class="n">share</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">c2w</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">images_train</span><span class="p">,</span> <span class="n">c2ws_train</span><span class="p">)):</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">add_camera_frustum</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">/cameras/</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">fov</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="n">H</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span>
        <span class="n">aspect</span><span class="o">=</span><span class="n">W</span> <span class="o">/</span> <span class="n">H</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>
        <span class="n">wxyz</span><span class="o">=</span><span class="n">viser</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="n">SO3</span><span class="p">.</span><span class="nf">from_matrix</span><span class="p">(</span><span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]).</span><span class="n">wxyz</span><span class="p">,</span>
        <span class="n">position</span><span class="o">=</span><span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
        <span class="n">image</span><span class="o">=</span><span class="n">image</span>
    <span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">rays_o</span><span class="p">,</span> <span class="n">rays_d</span><span class="p">)):</span>
    <span class="n">server</span><span class="p">.</span><span class="nf">add_spline_catmull_rom</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">/rays/</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">positions</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">((</span><span class="n">o</span><span class="p">,</span> <span class="n">o</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="mf">6.0</span><span class="p">)),</span>
    <span class="p">)</span>
<span class="n">server</span><span class="p">.</span><span class="nf">add_point_cloud</span><span class="p">(</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">/samples</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">colors</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">points</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">point_size</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Wait to allow visualization to run
</span></code></pre></div></div> <p><img src="/hws/hw4/assets/viser_plot.png" alt="viser plot" /></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Visualize Cameras, Rays and Samples
</span><span class="kn">import</span> <span class="n">viser</span><span class="p">,</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># --- You Need to Implement These ------
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nc">RaysData</span><span class="p">(</span><span class="n">images_train</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">c2ws_train</span><span class="p">)</span>

<span class="c1"># This will check that your uvs aren't flipped
</span><span class="n">uvs_start</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">uvs_end</span> <span class="o">=</span> <span class="mi">40_000</span>
<span class="n">sample_uvs</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">uvs</span><span class="p">[</span><span class="n">uvs_start</span><span class="p">:</span><span class="n">uvs_end</span><span class="p">]</span> <span class="c1"># These are integer coordinates of widths / heights (xy not yx) of all the pixels in an image
# uvs are array of xy coordinates, so we need to index into the 0th image tensor with [0, height, width], so we need to index with uv[:,1] and then uv[:,0]
</span><span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">all</span><span class="p">(</span><span class="n">images_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">sample_uvs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">sample_uvs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span> <span class="o">==</span> <span class="n">dataset</span><span class="p">.</span><span class="n">pixels</span><span class="p">[</span><span class="n">uvs_start</span><span class="p">:</span><span class="n">uvs_end</span><span class="p">])</span>

<span class="c1"># # Uncoment this to display random rays from the first image
# indices = np.random.randint(low=0, high=40_000, size=100)
</span>
<span class="c1"># # Uncomment this to display random rays from the top left corner of the image
# indices_x = np.random.randint(low=100, high=200, size=100)
# indices_y = np.random.randint(low=0, high=100, size=100)
# indices = indices_x + (indices_y * 200)
</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">rays_o</span><span class="sh">"</span><span class="p">:</span> <span class="n">dataset</span><span class="p">.</span><span class="n">rays_o</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="sh">"</span><span class="s">rays_d</span><span class="sh">"</span><span class="p">:</span> <span class="n">dataset</span><span class="p">.</span><span class="n">rays_d</span><span class="p">[</span><span class="n">indices</span><span class="p">]}</span>
<span class="n">points</span> <span class="o">=</span> <span class="nf">sample_along_rays</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">rays_o</span><span class="sh">"</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">rays_d</span><span class="sh">"</span><span class="p">],</span> <span class="n">random</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># ---------------------------------------
</span>
<span class="n">server</span> <span class="o">=</span> <span class="n">viser</span><span class="p">.</span><span class="nc">ViserServer</span><span class="p">(</span><span class="n">share</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">c2w</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">images_train</span><span class="p">,</span> <span class="n">c2ws_train</span><span class="p">)):</span>
  <span class="n">server</span><span class="p">.</span><span class="nf">add_camera_frustum</span><span class="p">(</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">/cameras/</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">fov</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">arctan2</span><span class="p">(</span><span class="n">H</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="n">aspect</span><span class="o">=</span><span class="n">W</span> <span class="o">/</span> <span class="n">H</span><span class="p">,</span>
    <span class="n">scale</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>
    <span class="n">wxyz</span><span class="o">=</span><span class="n">viser</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="n">SO3</span><span class="p">.</span><span class="nf">from_matrix</span><span class="p">(</span><span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]).</span><span class="n">wxyz</span><span class="p">,</span>
    <span class="n">position</span><span class="o">=</span><span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="n">image</span><span class="o">=</span><span class="n">image</span>
  <span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">rays_o</span><span class="sh">"</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">rays_d</span><span class="sh">"</span><span class="p">])):</span>
  <span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">stack</span><span class="p">((</span><span class="n">o</span><span class="p">,</span> <span class="n">o</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="mf">6.0</span><span class="p">))</span>
  <span class="n">server</span><span class="p">.</span><span class="nf">add_spline_catmull_rom</span><span class="p">(</span>
      <span class="sa">f</span><span class="sh">"</span><span class="s">/rays/</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">positions</span><span class="o">=</span><span class="n">positions</span><span class="p">,</span>
  <span class="p">)</span>
<span class="n">server</span><span class="p">.</span><span class="nf">add_point_cloud</span><span class="p">(</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">/samples</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">colors</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">points</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">points</span><span class="o">=</span><span class="n">points</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">point_size</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Wait to allow visualization to run
</span></code></pre></div></div> <p><img src="/hws/hw4/assets/single-camera-1.png" alt="viser plot of a single image (v1)" /></p> <p><img src="/hws/hw4/assets/single-camera-2.png" alt="viser plot of a single image (v2)" /></p> <p><img src="/hws/hw4/assets/upper-left-single-camera.png" alt="viser plot of top left rays in a single image" /></p><hr /> <h2 id="part-24-neural-radiance-field"> <a href="#part-24-neural-radiance-field" class="anchor-heading" aria-labelledby="part-24-neural-radiance-field"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 2.4: Neural Radiance Field </h2> <p><strong>[Impl: Network]</strong> After having samples in 3D, we want to use the network to predict the density and color for those samples in 3D. So you would create a MLP that is similar to Part 1, but with three changes:</p> <ul> <li>Input is now 3D world coordinates instead of 2D pixel coordinates, along side a 3D vector as the ray direction. And we are going to output not only the color, but also the density for the 3D points. In the radiance field, the color of each point depends on the view direction, so we are going to use the view direction as the condition when we predict colors. Note we use Sigmoid to constrain the output color within range (0, 1), and use ReLU to constrain the output density to be positive. The ray direction also needs to be encoded by positional encoding (PE) but can use less frequency (e.g., L=4) than the cooridnate PE (e.g., L=10).</li> <li>Make the MLP deeper. We are now doing a more challenging task of optimizing a 3D representation instead of 2D. So we need a more powerful network.</li> <li>Inject the input (after PE) to the middle of your MLP through concatenation. It’s a general trick for <em>deep</em> neural network, that is helpful for it to not forgetting about the input.</li> </ul> <p>Below is a structure of the network that you can start with:</p> <p><img src="/hws/hw4/assets/mlp_nerf.png" alt="MLP" /></p><hr /> <h2 id="part-25-volume-rendering"> <a href="#part-25-volume-rendering" class="anchor-heading" aria-labelledby="part-25-volume-rendering"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 2.5: Volume Rendering </h2> <p>The core volume rendering equation is as follows:</p> \[C(\mathbf{r})=\int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) dt, \text{ where } T(t)=\exp \left(-\int_{t_n}^t \sigma(\mathbf{r}(s)) ds\right)\] <p>This fundamentally means that at every small step \(dt\) along the ray, we add the contribution of that small interval \([t, t + dt]\) to that final color, and we do the infinitely many additions of these infinitesimally small intervals with an integral.</p> <p>The discrete approximation (thus tractable to compute) of this equation can be stated as the following: \(\hat{C}(\mathbf{r})=\sum_{i=1}^N T_i\left(1-\exp \left(-\sigma_i \delta_i\right)\right) \mathbf{c}_i, \text { where } T_i=\exp \left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)\) where \(\mathbf{c}_i\) is the color obtained from our network at sample location \(i\), \(T_i\) is the probability of a ray <em>not</em> terminating before sample location \(i\), and \(1 - e^{-\sigma_i \delta_i}\) is the probability of terminating at sample location \(i\).</p> <p>If your volume rendering works, the following snippet of code should pass the assert statement:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">sigmas</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">rgbs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="p">(</span><span class="mf">6.0</span> <span class="o">-</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">64</span>
<span class="n">rendered_colors</span> <span class="o">=</span> <span class="nf">volrend</span><span class="p">(</span><span class="n">sigmas</span><span class="p">,</span> <span class="n">rgbs</span><span class="p">,</span> <span class="n">step_size</span><span class="p">)</span>

<span class="n">correct</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.5006</span><span class="p">,</span> <span class="mf">0.3728</span><span class="p">,</span> <span class="mf">0.4728</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4322</span><span class="p">,</span> <span class="mf">0.3559</span><span class="p">,</span> <span class="mf">0.4134</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4027</span><span class="p">,</span> <span class="mf">0.4394</span><span class="p">,</span> <span class="mf">0.4610</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4514</span><span class="p">,</span> <span class="mf">0.3829</span><span class="p">,</span> <span class="mf">0.4196</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4002</span><span class="p">,</span> <span class="mf">0.4599</span><span class="p">,</span> <span class="mf">0.4103</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4471</span><span class="p">,</span> <span class="mf">0.4044</span><span class="p">,</span> <span class="mf">0.4069</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4285</span><span class="p">,</span> <span class="mf">0.4072</span><span class="p">,</span> <span class="mf">0.3777</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4152</span><span class="p">,</span> <span class="mf">0.4190</span><span class="p">,</span> <span class="mf">0.4361</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4051</span><span class="p">,</span> <span class="mf">0.3651</span><span class="p">,</span> <span class="mf">0.3969</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.3253</span><span class="p">,</span> <span class="mf">0.3587</span><span class="p">,</span> <span class="mf">0.4215</span><span class="p">]</span>
<span class="p">])</span>
<span class="k">assert</span> <span class="n">torch</span><span class="p">.</span><span class="nf">allclose</span><span class="p">(</span><span class="n">rendered_colors</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
</code></pre></div></div> <p><strong>[Impl]</strong> Here you will implement the volume rendering equation for a batch of samples along a ray. This rendered color is what we will compare with our posed images in order to train our network. You would need to implement this part in torch instead of numpy because we need the loss to be able to backpropagate through this part. A hint is that you may find <a href="https://pytorch.org/docs/stable/generated/torch.cumsum.html">torch.cumsum</a> or <a href="https://pytorch.org/docs/stable/generated/torch.cumprod.html">torch.cumprod</a> useful here.</p> <p><strong>[Deliverables]</strong> As a reference, the images below show the process of optimizing the network to fit on our lego multi-view images from a novel view. The staff solution reaches above 23 PSNR with 1000 gradient steps and a batchsize of 10K rays per gradent step. The staff solution uses an Adam optimizer with a learning rate of 5e-4. For guaranteed full credit, achieve 23 PSNR for any number of iterations.</p> <p><img src="/hws/hw4/assets/nerf_training.jpg" alt="training" /></p> <ul> <li>Include a brief description of how you implement each part.</li> <li>Report the visualization of the rays and samples you draw at a single training step (along with the cameras), similar to the plot we show above. Plot up to 100 rays to make it less crowded.</li> <li>Visualize the training process by plotting the predicted images across iterations, similar to the above reference, as well as the PSNR curve on the validation set (6 images).</li> <li>After you train the network, you can use it to render a novel view image of the lego from arbitrary camera extrinsic. Show a spherical rendering of the lego video using the provided cameras extrinsics (<code class="language-plaintext highlighter-rouge">c2ws_test</code> in the npz file). You should be get a video like this (left is 10 after minutes training, right is 2.5 hrs training):</li> </ul> <video width="320" height="320" autoplay="" muted="" loop="" style="display: inline-block"> <source type="video/mp4" src="/hws/hw4/assets/lego.mp4" /> </video> <video width="320" height="320" autoplay="" muted="" loop="" style="display: inline-block"> <source type="video/mp4" src="/hws/hw4/assets/video_2.5h.mp4" /> </video><hr /> <h2 id="part-26-training-with-your-own-data"> <a href="#part-26-training-with-your-own-data" class="anchor-heading" aria-labelledby="part-26-training-with-your-own-data"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 2.6: Training with your own data </h2> <p>You will now use the dataset you created in part 0 to create a NeRF of your chosen object. After training a NeRF on your dataset render a gif of novel views from your scene. We have provided some starter code below which may be useful.</p> <p><strong>[UPDATE 11/14/2025]</strong></p> <p>The calibrated Lafufu Dataset can be found <a href="/hws/hw4/assets/lafufu_dataset.npz">here</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">look_at_origin</span><span class="p">(</span><span class="n">pos</span><span class="p">):</span>
  <span class="c1"># Camera looks towards the origin
</span>  <span class="n">forward</span> <span class="o">=</span> <span class="o">-</span><span class="n">pos</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span>  <span class="c1"># Normalize the direction vector
</span>
  <span class="c1"># Define up vector (assuming y-up)
</span>  <span class="n">up</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

  <span class="c1"># Compute right vector using cross product
</span>  <span class="n">right</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cross</span><span class="p">(</span><span class="n">up</span><span class="p">,</span> <span class="n">forward</span><span class="p">)</span>
  <span class="n">right</span> <span class="o">=</span> <span class="n">right</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">right</span><span class="p">)</span>

  <span class="c1"># Recompute up vector to ensure orthogonality
</span>  <span class="n">up</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cross</span><span class="p">(</span><span class="n">forward</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span>

  <span class="c1"># Create the camera-to-world matrix
</span>  <span class="n">c2w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
  <span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">right</span>
  <span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">up</span>
  <span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">forward</span>
  <span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">pos</span>

  <span class="k">return</span> <span class="n">c2w</span>

<span class="k">def</span> <span class="nf">rot_x</span><span class="p">(</span><span class="n">phi</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
        <span class="p">[</span><span class="n">math</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">phi</span><span class="p">),</span> <span class="o">-</span><span class="n">math</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">phi</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="n">math</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">phi</span><span class="p">),</span> <span class="n">math</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">phi</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">])</span>

<span class="c1"># TODO: Change start position to a good position for your scene such as 
# the translation vector of one of your training camera extrinsics
</span><span class="n">START_POS</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
<span class="n">NUM_SAMPLES</span> <span class="o">=</span> <span class="mi">60</span>

<span class="n">frames</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">phi</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">360.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">NUM_SAMPLES</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">c2w</span> <span class="o">=</span> <span class="nf">look_at_origin</span><span class="p">(</span><span class="n">START_POS</span><span class="p">)</span>
    <span class="n">extrinsic</span> <span class="o">=</span> <span class="nf">rot_x</span><span class="p">(</span><span class="n">phi</span><span class="o">/</span><span class="mf">180.</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">@</span> <span class="n">c2w</span>
    
    <span class="c1"># Generate view for this camera pose
</span>    <span class="c1"># TODO: Add code for generating a view with your model from the current extrinsic
</span>    <span class="n">frame</span> <span class="o">=</span> <span class="bp">...</span>
    <span class="n">frames</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
</code></pre></div></div> <p>Helpful Tips / Common Mistakes:</p> <ul> <li>When using the test data our near and far parameters are set to 2.0 and 6.0 respectively. You will likely have to adjust these for the real data you collect. These parameters represent the minimum and maximum distance away from the camera’s sensor that we start and stop sampling. For our example we found that near = 0.02 and far = 0.5 worked well, but you will likely have to do some experimenting to find values that work for you.</li> <li>You might want to increase the number of samples along your rays for your real data. This will take longer to train, but can improve visual quality of your NeRF. For our implementation we first trained with 32 samples in order to ensure that there are no issues or bugs in other parts of our code and then increased to 64 samples per ray to get our final result.</li> <li>If training is taking an unreasonable amount of time, your image resolution may be the issue. Attempting to train with too large of images may take a long time. If you resize your images you need to ensure that your intrinsics matrix reflects this change either by resizing before doing calibration or adjusting the intrinsics matrix after recovering it.</li> </ul> <p><strong>[Impl]</strong> Train a NeRF on your chosen object dataset collected in part 0. Make sure to save the training loss over iterations as well as to generate intermediate renders for the deliverables.</p> <p><strong>[Deliverables]</strong> Create a gif of a camera circling the object showing novel views and discuss any code or hyperparameter changes you had to make. Include a plot of the training loss as well as some intermediate renders of the scene while it is training.</p> <p><img src="/hws/hw4/assets/labubu_nerf.gif" alt="Labubu NeRF" /></p> <p><img src="/hws/hw4/assets/labubu_train_500.png" alt="Labubu Training at 500 iterations" /> <img src="/hws/hw4/assets/labubu_train_2000.png" alt="Labubu Training at 2000 iterations" /> <img src="/hws/hw4/assets/labubu_train_6000.png" alt="Labubu Training at 6000 iterations" /></p> <p><img src="/hws/hw4/assets/labubu_stats.png" alt="Labubu Training Loss Statistics" /></p><hr /> <h2 id="bells--whistles"> <a href="#bells--whistles" class="anchor-heading" aria-labelledby="bells--whistles"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Bells &amp; Whistles </h2> <h3 id="required-for-cs-280a-students-only"> <a href="#required-for-cs-280a-students-only" class="anchor-heading" aria-labelledby="required-for-cs-280a-students-only"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Required for CS 280A students only: </h3> <ul> <li><strong>Render the depths map video for the Lego scene.</strong> Instead of compositing per-point colors to the pixel color in the volume rendering, we can also composite per-point depths to the pixel depth. (See the reference video below)</li> </ul> <h3 id="optional-for-all-students"> <a href="#optional-for-all-students" class="anchor-heading" aria-labelledby="optional-for-all-students"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Optional for all students: </h3> <p>The following are optional explorations for any students interested in going deeper with NeRF.</p> <ul> <li>Better (more efficient) sampling: Implement course-to-fine PDF resampling as described in the original <a href="https://www.matthewtancik.com/nerf">NeRF paper</a>.</li> <li>Better NeRF representations: Replace MLP with something more advanced to make it faster. (e.g. <a href="https://apchenstu.github.io/TensoRF/">TensoRF</a> or <a href="https://nvlabs.github.io/instant-ngp/">Instant-NGP</a>). For this part it is ok to borrow some code from existing implementations (mark reference!) to your code base and see how it affect your NeRF optimization.</li> <li>Improve PSNR to 30+: Aside from better sampling, better NeRF representations, try other things you can think of to improve the quality of the images to get 30+ db in PSNR.</li> <li>Render the Lego video with a different background color than black. You would need to revisit the volume rendering equation to see where you should inject the background color.</li> <li>Implement scene contraction for large scenes as specified in <a href="https://jonbarron.info/mipnerf360/">Mip-NeRF 360</a>. This allows NeRF to handle unbounded scenes by contracting distant points into a bounded space.</li> <li>Use <a href="https://docs.nerf.studio/">nerfstudio</a> to make a cool video!</li> </ul> <video width="640" height="320" autoplay="" muted="" loop="" style="display: block; margin-left: auto; margin-right: auto"> <source type="video/mp4" src="/hws/hw4/assets/depths.mp4" /> </video><hr /> <h1 id="deliverables-checklist"> <a href="#deliverables-checklist" class="anchor-heading" aria-labelledby="deliverables-checklist"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Deliverables Checklist </h1> <p>Make sure your submission includes all of the following:</p> <ul> <li>Submit your webpage public URL to the class gallery by filling out <a href="https://forms.gle/nbpg6wATeCDU5sj49">this</a> form.</li> </ul> <h2 id="part-0-camera-calibration-and-3d-scanning"> <a href="#part-0-camera-calibration-and-3d-scanning" class="anchor-heading" aria-labelledby="part-0-camera-calibration-and-3d-scanning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 0: Camera Calibration and 3D Scanning </h2> <ul> <li>2 screenshots of your camera frustums visualization in Viser</li> </ul> <h2 id="part-1-fit-a-neural-field-to-a-2d-image-1"> <a href="#part-1-fit-a-neural-field-to-a-2d-image-1" class="anchor-heading" aria-labelledby="part-1-fit-a-neural-field-to-a-2d-image-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 1: Fit a Neural Field to a 2D Image </h2> <ul> <li>Model architecture report (number of layers, width, learning rate, and other important details)</li> <li>Training progression visualization on both the provided test image and one of your own images</li> <li>Final results for 2 choices of max positional encoding frequency and 2 choices of width (2x2 grid)</li> <li>PSNR curve for training on one image of your choice</li> </ul> <h2 id="part-2-fit-a-neural-radiance-field-from-multi-view-images-1"> <a href="#part-2-fit-a-neural-radiance-field-from-multi-view-images-1" class="anchor-heading" aria-labelledby="part-2-fit-a-neural-radiance-field-from-multi-view-images-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 2: Fit a Neural Radiance Field from Multi-view Images </h2> <ul> <li>Brief description of how you implemented each part</li> <li>Visualization of rays and samples with cameras (up to 100 rays)</li> <li>Training progression visualization with predicted images across iterations</li> <li>PSNR curve on the validation set</li> <li>Spherical rendering video of the Lego using provided test cameras</li> </ul> <h2 id="part-26-training-with-your-own-data-1"> <a href="#part-26-training-with-your-own-data-1" class="anchor-heading" aria-labelledby="part-26-training-with-your-own-data-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 2.6: Training with Your Own Data </h2> <ul> <li>GIF of camera circling your object showing novel views</li> <li>Discussion of code or hyperparameter changes you made</li> <li>Plot of training loss over iterations</li> <li>Intermediate renders of the scene during training</li> </ul> <h2 id="bells--whistles-if-applicable"> <a href="#bells--whistles-if-applicable" class="anchor-heading" aria-labelledby="bells--whistles-if-applicable"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Bells &amp; Whistles (if applicable) </h2> <ul> <li><strong>CS 280A students:</strong> Depth map video for the Lego scene</li> <li><strong>Optional:</strong> Any additional explorations you completed</li> </ul> </main> <hr> <footer> <div class="d-xs-block d-md-none"> <div class="mt-4 fs-2"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </div> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
