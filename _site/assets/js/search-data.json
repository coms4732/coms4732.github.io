{"0": {
    "doc": "Homework 1",
    "title": "\n    Homework 1\n    \n    COMS4732: Computer Vision 2\n  ",
    "content": ". ",
    "url": "/hw1/",
    
    "relUrl": "/hw1/"
  },"1": {
    "doc": "Homework 1",
    "title": "\n  \n  Images of the Russian Empire:\n  Colorizing the Prokudin-Gorskii photo collection\n  Due Date: TBD\n",
    "content": " ",
    "url": "/hw1/",
    
    "relUrl": "/hw1/"
  },"2": {
    "doc": "Homework 1",
    "title": "Background",
    "content": "Sergei Mikhailovich Prokudin-Gorskii (1863-1944) [Сергей Михайлович Прокудин-Горский, to his Russian friends] was a man well ahead of his time. Convinced, as early as 1907, that color photography was the wave of the future, he won Tzar’s special permission to travel across the vast Russian Empire and take color photographs of everything he saw including the only color portrait of Leo Tolstoy. And he really photographed everything: people, buildings, landscapes, railroads, bridges… thousands of color pictures! His idea was simple: record three exposures of every scene onto a glass plate using a red, a green, and a blue filter. Never mind that there was no way to print color photographs until much later – he envisioned special projectors to be installed in “multimedia” classrooms all across Russia where the children would be able to learn about their vast country. Alas, his plans never materialized: he left Russia in 1918, right after the revolution, never to return again. Luckily, his RGB glass plate negatives, capturing the last years of the Russian Empire, survived and were purchased in 1948 by the Library of Congress. The LoC has recently digitized the negatives and made them available on-line. ",
    "url": "/hw1/#background",
    
    "relUrl": "/hw1/#background"
  },"3": {
    "doc": "Homework 1",
    "title": "Overview",
    "content": "The goal of this assignment is to take the digitized Prokudin-Gorskii glass plate images and, using image processing techniques, automatically produce a color image with as few visual artifacts as possible. In order to do this, you will need to extract the three color channel images, place them on top of each other, and align them so that they form a single RGB color image. This is a cool explanation on how the Library of Congress composed their color images. Some starter code is available in Python; do not feel obligated to use it. We will assume that a simple x,y translation model is sufficient for proper alignment. However, the full-size glass plate images (i.e.tif files) are very large, so your alignment procedure will need to be relatively fast and efficient. When you begin your naive implementation, you should start with the smaller files monastery.jpg and cathedral.jpg provided, or by downsizing the larger files. Your submission should be ran on the full-size images. ",
    "url": "/hw1/#overview",
    
    "relUrl": "/hw1/#overview"
  },"4": {
    "doc": "Homework 1",
    "title": "Details",
    "content": ". A few of the digitized glass plate images (both hi-res and low-res versions) will be placed in the following zip file (note that the filter order from top to bottom is BGR, not RGB!): data.zip (online gallery for preview). Your program will take a glass plate image as input and produce a single color image as output. The program should divide the image into three equal parts and align the second and the third parts (e.x. G and R) to the first (B). For each image, you will need to print the (x,y) displacement vector that was used to align the parts. The easiest way to align the parts is to exhaustively search over a window of possible displacements (say [-15,15] pixels), score each one using some image matching metric, and take the displacement with the best score. There is a number of possible metrics that one could use to score how well the images match. The simplest one is just the L2 norm also known as the Euclidean Distance which is simply sqrt(sum(sum((image1-image2).^2))) where the sum is taken over the pixel values. Another is Normalized Cross-Correlation (NCC), which is simply a dot product between two normalized vectors: (image1./||image1|| and image2./||image2||). Exhaustive search will become prohibitively expensive if the pixel displacement is too large (which will be the case for high-resolution glass plate scans). In this case, you will need to implement a faster search procedure such as an image pyramid. An image pyramid represents the image at multiple scales (usually scaled by a factor of 2) and the processing is done sequentially starting from the coarsest scale (smallest image) and going down the pyramid, updating your estimate as you go. It is very easy to implement by adding recursive calls to your original single-scale implementation. You should implement the pyramid functionality yourself using appropriate downsampling techniques. Your job will be to implement an algorithm that, given a 3-channel image, produces a color image as output. Implement a simple single-scale version first, using for loops, searching over a user-specified window of displacements. The above directory has skeleton Python code that will help you get started and you should pick one of the smaller .jpg images in the directory to test this version of the code. Next, add a coarse-to-fine pyramid speedup to handle large images like the .tif ones provided in the directory. Note that in the case like the Emir of Bukhara (show on right), the images to be matched do not actually have the same brightness values (they are different color channels), so you might have to use a cleverer metric, or different features than the raw pixels. This image is a great candidate for a Bells &amp; Whistles extension if you want to explore more advanced alignment strategies or heuristics. However, for grading, we allow up to one image (out of the original 14, excluding your own) to be misaligned in your final results; aim to get the rest properly aligned. ",
    "url": "/hw1/#details",
    
    "relUrl": "/hw1/#details"
  },"5": {
    "doc": "Homework 1",
    "title": "Bells &amp; Whistles (TBD IF TO ASSIGN OR NOT)",
    "content": "Although the color images resulting from this automatic procedure will often look strikingly real, they are still a far cry from the manually restored versions available on the LoC website and from other professional photographers. Of course, each such photograph takes days of painstaking Photoshop work, adjusting the color levels, removing the blemishes, adding contrast, etc. Can we make some of these adjustments automatically, without the human in the loop? . You can use any libraries to solve bells and whistles as long as you can explain what it is doing and why you used it. | Automatic cropping. Remove white, black or other color borders. Don’t just crop a predefined margin off of each side – actually try to detect the borders or the edge between the border and the image. | Automatic contrasting. It is usually safe to rescale image intensities such that the darkest pixel is zero (on its darkest color channel) and the brightest pixel is 1 (on its brightest color channel). More drastic or non-linear mappings may improve perceived image quality. | Automatic white balance. This involves two problems – 1) estimating the illuminant and 2) manipulating the colors to counteract the illuminant and simulate a neutral illuminant. Step 1 is difficult in general, while step 2 is simple (see the Wikipedia page on Color Balance and section 2.3.2 in the Szeliski book). There exist some simple algorithms for step 1, which don’t necessarily work well – assume that the average color or the brightest color is the illuminant and shift those to gray or white. | Better color mapping. There is no reason to assume (as we have) that the red, green, and blue lenses used by Produkin-Gorskii correspond directly to the R, G, and B channels in RGB color space. Try to find a mapping that produces more realistic colors (and perhaps makes the automatic white balancing less necessary). | Better features. Instead of aligning based on RGB similarity, try using gradients or edges. | . (Optional) Feel free to come up with your own approaches. There is no right answer here – just try out things and see what works. For example, the borders of the photograph will have strange colors since the three channels won’t exactly align. See if you can devise an automatic way of cropping the border to get rid of the bad stuff. One possible idea is that the information in the good parts of the image generally agrees across the color channels, whereas at borders it does not. | Better transformations. Instead of searching for the best x and y translation, additionally search over small scale changes and rotations. Adding two more dimensions to your search will slow things down, but the same course to fine progression should help alleviate this. | Aligning and processing data from other sources. In many domains, such as astronomy, image data is still captured one channel at a time. Often the channels don’t correspond to visible light, but NASA artists stack these channels together to create false color images. For example, this tutorial on how to process Hubble Space Telescope imagery yourself. Also, consider images like this one of a coronal mass ejection built by combining ultraviolet images from the Solar Dynamics Observatory. To truly show that your algorithm works, you should demonstrate a non-trivial alignment and color correction that your algorithm found. | . ",
    "url": "/hw1/#bells--whistles-tbd-if-to-assign-or-not",
    
    "relUrl": "/hw1/#bells--whistles-tbd-if-to-assign-or-not"
  },"6": {
    "doc": "Homework 1",
    "title": "Deliverables",
    "content": "For this project, you must submit both your code and a project webpage as described here. The project webpage is your presentation of your work. Imagine that you are writing a blog post about your project for your friends. A good blog post is easy to read and follow, well organized, and visually appealing. When you introduce new concepts or tricks that improve your results, explain them along the way and show the improved results of your algorithm on example images. Below are the specific deliverables to keep in mind when writing your project webpage. | The results of a single-scale alignment (using NCC/L2 norm metrics) on the low-resolution images (JPEG files). | The results of a multi-scale pyramid alignment (using NCC/L2 norm metrics) on all of our example images. List the offsets you computed. | The results of your algorithm (using NCC/L2 norm metrics) on a few examples of your choosing, downloaded from the Prokudin-Gorskii collection. | If your algorithm failed to align any image, provide a brief explanation of why. | Describe any bells and whistles you implemented. For maximum credit, show before-and-after images. | Submit your webpage URL to the class gallery via Google Form. Also include this URL in your Gradescope submission. | . Important: Images are for the project webpage only. Do not upload image files (e.g., .jpg, .png, .tif) to Gradescope. This keeps submissions small and avoids hitting Gradescope’s 100 MB upload limit, which large image sets can easily exceed. ",
    "url": "/hw1/#deliverables",
    
    "relUrl": "/hw1/#deliverables"
  },"7": {
    "doc": "Homework 1",
    "title": "Final Advice",
    "content": ". | You’ll build image pyramids again in Project 2—write clean, reusable code. | Implement almost everything from scratch. It’s fine to use functions for reading, writing, resizing, shifting, and displaying images (e.g., imread, imresize, circshift), but don’t use high‑level functions for Laplacian/Gaussian pyramids, automatic alignment, etc. If in doubt, ask on Ed. | Aim for under 1 minute per image. If it takes hours, optimize. | Vectorize/parallelize and avoid many for‑loops. See Python performance tips and NumPy broadcasting: Python · NumPy Broadcasting. | Use a fixed set of parameters; don’t over‑tune per image. One failure is okay with simple metrics. | Convert images to floats and the same scale (e.g., im2double/im2uint8). JPGs are uint8; TIFFs may be uint16. | Shift arrays with np.roll. | Ignore borders when scoring; compute metrics on interior pixels. | Save outputs as JPG to reduce disk usage. | . ",
    "url": "/hw1/#final-advice",
    
    "relUrl": "/hw1/#final-advice"
  },"8": {
    "doc": "Homework 1",
    "title": "Grading Rubric",
    "content": "This assignment will be graded out of 100 points, as follows: . Single-scale alignment (60 points total) . For results: . | +50%: No alignment defects on \"cathedral,\" \"monastery,\" \"tobolsk.\" (Assume single‑scale is satisfied if pyramids work.) | +30%: Some defects on \"cathedral,\" \"monastery,\" \"tobolsk.\" | 0%: No effort on alignment. | . For presentation: . | +50%: Thorough explanation / approach / good presentation. | +30%: Explanation present, could go further in depth. | +20%: Minimal explanation on webpage. | 0%: No section / no explanation on webpage. | . Multi-scale pyramid alignment (with NCC / L2) (40 points total) . For results: . | +50%: Alignment defects on ≤ 1 / 14 images. | +40%: Alignment defects on ≤ 3 / 14 images, or missing. | +30%: Alignment defects on ≤ 6 / 14 images, or missing. | +20%: Alignment defects on &gt; 6 / 14 images, but effort shown. | 0%: No effort on alignment. | . For presentation: . | +50%: Thorough explanation, walking through each step carefully (e.g., NCC / L2). | +40%: Explains motivation / approach / good presentation. | +30%: Explanation present, could go further in depth. | +20%: Minimal explanation on webpage. | 0%: No explanation on webpage. | . ",
    "url": "/hw1/#grading-rubric",
    
    "relUrl": "/hw1/#grading-rubric"
  },"9": {
    "doc": "Homework 1",
    "title": "Common Questions",
    "content": "Q: What’s considered a good alignment vs. a bad alignment? . Since one failure is allowed while still receiving full credit for alignment, aim for strong results on most images (with a few failures) rather than acceptable-but-mediocre results on all images. | Okay | Not Okay | . | | | . Q: What if I used a better distance function beyond L2 and NCC to get better alignments? . That’s great and encouraged. However, to receive full credit you must still document results using the basic distance functions (L2/NCC). If you skip this, your presentation score will be penalized. ",
    "url": "/hw1/#common-questions",
    
    "relUrl": "/hw1/#common-questions"
  },"10": {
    "doc": "Homework 1",
    "title": "Homework 1",
    "content": " ",
    "url": "/hw1/",
    
    "relUrl": "/hw1/"
  },"11": {
    "doc": "Homework 2",
    "title": "\n    Homework 2\n    COMS4732: Computer Vision 2\n  ",
    "content": ". ",
    "url": "/hw2/",
    
    "relUrl": "/hw2/"
  },"12": {
    "doc": "Homework 2",
    "title": "\n  \n    \n    \n  \n  AUTOMATIC FEATURE MATCHING ACROSS IMAGES\n  Due Date: TBD\n",
    "content": " ",
    "url": "/hw2/",
    
    "relUrl": "/hw2/"
  },"13": {
    "doc": "Homework 2",
    "title": "Background",
    "content": "This assignment will involve creating a system for automatically detecting corresponding features in 2 images, as well as learning how to read and implement a research paper. We will follow the paper “Multi-Image Matching using Multi-Scale Oriented Patches” by Brown et al. but with several simplifications. Read the paper first and make sure you understand it, then implement the algorithm. ",
    "url": "/hw2/#background",
    
    "relUrl": "/hw2/#background"
  },"14": {
    "doc": "Homework 2",
    "title": "Step 1: Harris Corner Detection (5 points)",
    "content": ". | Start with Harris Interest Point Detector (Section 2). We won’t worry about multi-scale – just do a single scale. Also, don’t worry about sub-pixel accuracy. Re-implementing Harris is a thankless task – so you can use our sample code: harris.py. | . Deliverables: Show your 2 images, as-is, side-by-side. Also, show detected corners overlaid on your set of images side-by-side. ",
    "url": "/hw2/#step-1-harris-corner-detection-5-points",
    
    "relUrl": "/hw2/#step-1-harris-corner-detection-5-points"
  },"15": {
    "doc": "Homework 2",
    "title": "Step 2: Adaptive Non-Maximal Suppression (ANMS) (15 points)",
    "content": ". | Implement Adaptive Non-Maximal Suppression (ANMS, Section 3). Include in your submission a figure of the chosen corners overlaid on the image. This section has multiple moving parts; you may need to read it a few times. You may want to skip this step and come back to it; just choose a random set of corners instead in the meantime. | . Deliverables: Show chosen corners overlaid on your set of images side-by-side after applying ANMS. ",
    "url": "/hw2/#step-2-adaptive-non-maximal-suppression-anms-15-points",
    
    "relUrl": "/hw2/#step-2-adaptive-non-maximal-suppression-anms-15-points"
  },"16": {
    "doc": "Homework 2",
    "title": "Step 3: Feature Descriptor Extraction (5 points)",
    "content": "Implement Feature Descriptor extraction (Section 4 of the paper). Don’t worry about rotation-invariance – just extract axis-aligned 8x8 patches. Note that it’s extremely important to sample these patches from the larger 40x40 window to have a nice big blurred descriptor. Don’t forget to bias/gain-normalize the descriptors. Ignore the wavelet transform section. Deliverables: deliverable is part of step 4 . ",
    "url": "/hw2/#step-3-feature-descriptor-extraction-5-points",
    
    "relUrl": "/hw2/#step-3-feature-descriptor-extraction-5-points"
  },"17": {
    "doc": "Homework 2",
    "title": "Step 4: Feature Matching (15 points)",
    "content": "Implement Feature Matching (Section 5 of the paper). That is, you will need to find pairs of features that look similar and are thus likely to be good matches. There are 2 approaches for this: . | Nearest neighbor matching: Find the nearest neighbor for each feature in the other image and check if the distance is less than some threshold. | Lowe’s ratio test / NN distance ratio (NNDR): . | Threshold on the ratio between the first and the second nearest neighbors. Consult Figure 6b in the paper for picking the threshold. Ignore Section 6 of the paper. | The idea here is that a good descriptor from img2 should be significantly better than the other img2 descriptor candidates while still being close to the img1 descriptor. | . | . You will implement the latter: for your set of images, display the NNDR histogram and highlight the threshold you used. Deliverables: . | Display the NNDR histogram and highlight the threshold you used. Also specify which similarity metric you used (e.g. SSD, NCC, etc.). | Visualize the 5 best feature matches between the 2 images (no worries if you don’t have 10 matches, just show as many as you can). | the first column should be the feature descriptor for img1’s feature. | the second column should be the 1NN feature descriptor from img2. | the third column should be the 2NN feature descriptor from img2. | . | Color-code the matched features across both images and display them side-by-side. Also, put a number next to each feature to indicate the match index. | . ",
    "url": "/hw2/#step-4-feature-matching-15-points",
    
    "relUrl": "/hw2/#step-4-feature-matching-15-points"
  },"18": {
    "doc": "Homework 2",
    "title": "Step 5: RANSAC to estimate the homography (40 points)",
    "content": ". | Use 4-point RANSAC as described in class to compute robust homography estimates. Using your best homography estimate, visualize the inliers among the homography applied on all points. | . Deliverables: Show the inliers associated with the best homography found with RANSAC. ",
    "url": "/hw2/#step-5-ransac-to-estimate-the-homography-40-points",
    
    "relUrl": "/hw2/#step-5-ransac-to-estimate-the-homography-40-points"
  },"19": {
    "doc": "Homework 2",
    "title": "Bells &amp; Whistles (Optional)",
    "content": ". | Image Panoramic: Create a panoramic image from the 2 images by stitching them together using the homography and blending the images together. | Rotation invariance: Add rotation invariance to the MOPS feature descriptors. | Support for 3+ images: Implement support for 3+ images by using the homography to stitch together more than 2 images. | . ",
    "url": "/hw2/#bells--whistles-optional",
    
    "relUrl": "/hw2/#bells--whistles-optional"
  },"20": {
    "doc": "Homework 2",
    "title": "Deliverables",
    "content": "You must submit your code and visualizations outlined above for 2 different scenes / pairs of images. You cannot use the example provided by the staff. An example for one scene is shown below: . ",
    "url": "/hw2/#deliverables",
    
    "relUrl": "/hw2/#deliverables"
  },"21": {
    "doc": "Homework 2",
    "title": "Hints",
    "content": ". | You can use LLMs to implement any visualization code you wish. | If you would like to visually debug using the staff example, the images can be found here: north1.jpg and north2.jpg | Some hyperparameters (such as the RANSAC NNDR threshold) may need to be tuned for each image pair. For references, the staff solution for the example provided in this webpage used the following: | . | Hyperparameter | Value | Description | . | harris_corner_edge_discard | 20 | Number of pixels to discard from image edges when detecting Harris corners | . | anms_c_robust | 0.8 | Robustness parameter for Adaptive Non-Maximal Suppression (ANMS) | . | anms_num_points | 250 | Number of interest points to retain after ANMS | . | feature_matching_metric | “NCC” | Metric for comparing feature descriptors (e.g., NCC, SSD) | . | feature_matching_ratio_threshold | 0.85 | NNDR (Nearest Neighbor Distance Ratio) threshold for feature matching | . | ransac_s | 4 | Number of correspondences to sample per RANSAC iteration | . | ransac_epsilon | 1.0 | Distance threshold (in pixels) for RANSAC inlier classification | . | ransac_num_iters | 15000 | Number of iterations to run RANSAC | . ",
    "url": "/hw2/#hints",
    
    "relUrl": "/hw2/#hints"
  },"22": {
    "doc": "Homework 2",
    "title": "Acknowledgements",
    "content": "This assignment is based on Alyosha Efros’s version at Berkeley. ",
    "url": "/hw2/#acknowledgements",
    
    "relUrl": "/hw2/#acknowledgements"
  },"23": {
    "doc": "Homework 2",
    "title": "Homework 2",
    "content": " ",
    "url": "/hw2/",
    
    "relUrl": "/hw2/"
  },"24": {
    "doc": "Homework 3",
    "title": "Homework 3",
    "content": "This is the specification page for Homework 3. ",
    "url": "/hw3/",
    
    "relUrl": "/hw3/"
  },"25": {
    "doc": "Homework 4",
    "title": "Homework 4",
    "content": "CS180: Intro to Computer Vision and Computational Photography . ",
    "url": "/hw4/#homework-4",
    
    "relUrl": "/hw4/#homework-4"
  },"26": {
    "doc": "Homework 4",
    "title": "Neural Radiance Field!",
    "content": " ",
    "url": "/hw4/#neural-radiance-field",
    
    "relUrl": "/hw4/#neural-radiance-field"
  },"27": {
    "doc": "Homework 4",
    "title": "Due Date: November 14th, 2025",
    "content": "Note on compute requirements: We’re using PyTorch to implement neural networks with GPU acceleration. If you have an M-series Mac (M1/M2/M3), you should be able to run everything locally using the MPS backend. For older or less powerful hardware, we recommend using GPUs from Colab. Colab Pro is now free for students. ",
    "url": "/hw4/#due-date-november-14th-2025",
    
    "relUrl": "/hw4/#due-date-november-14th-2025"
  },"28": {
    "doc": "Homework 4",
    "title": "Part 0: Calibrating Your Camera and Capturing a 3D Scan",
    "content": "For the first part of the assignment, you will take a 3D scan of your own object which you will build a NeRF of later! To do this, we will use visual tracking targets called ArUco tags, which provide a way to reliably detect the same 3D keypoints across different images. There are 2 parts: 1) calibrating your camera parameters, and 2) using them to estimate pose. This part can be done entirely locally on your laptop, no need for a GPU. We will be using many helper functions from OpenCV, which you can install with pip install opencv-python. ",
    "url": "/hw4/#part-0-calibrating-your-camera-and-capturing-a-3d-scan",
    
    "relUrl": "/hw4/#part-0-calibrating-your-camera-and-capturing-a-3d-scan"
  },"29": {
    "doc": "Homework 4",
    "title": "Part 0.1: Calibrating Your Camera",
    "content": "First, either print out the following calibration tags or pull them up on a laptop/iPad screen. Capture 30-50 images of these tags from your phone camera, keeping the zoom the same. While capturing, results will be best if you vary the angle/distance of the camera, like shown in chessboard calibration in lecture. Note: For the purpose of this assignment, phone cameras work better since they provide undistorted images without lens effects. If you use a real camera, try to make sure there are no distortion effects. Now you’ll write a script to calibrate your camera using the images you captured. Here’s the pipeline: . | Loop through all your calibration images | For each image, detect the ArUco tags using OpenCV’s ArUco detector | Extract the corner coordinates from the detected tags | Collect all detected corners and their corresponding 3D world coordinates (you can consider the ArUco tag as the world origin and define the 4 corners’ 3D points relative to that, e.g., if your tag is 0.02m × 0.02m, the corners could be [(0,0,0), (0.02,0,0), (0.02,0.02,0), (0,0.02,0)]) | Use cv2.calibrateCamera() to compute the camera intrinsics and distortion coefficients | . Important: Your code should handle cases where tags aren’t detected in some images (this is common). Make sure to check if tags were found before trying to use the detection results, otherwise your script will crash. Here’s a code snippet to get you started with detecting ArUco tags (the tags in the PDF are 4x4): . import cv2 import numpy as np # Create ArUco dictionary and detector parameters (4x4 tags) aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50) aruco_params = cv2.aruco.DetectorParameters() # Detect ArUco markers in an image # Returns: corners (list of numpy arrays), ids (numpy array) corners, ids, _ = cv2.aruco.detectMarkers(image, aruco_dict, parameters=aruco_params) # Check if any markers were detected if ids is not None: # Process the detected corners # corners: list of length N (number of detected tags) # - each element is a numpy array of shape (1, 4, 2) containing the 4 corner coordinates (x, y) # ids: numpy array of shape (N, 1) containing the tag IDs for each detected marker # Example: if 3 tags detected, corners will be a list of 3 arrays, ids will be shape (3, 1) else: # No tags detected in this image, skip it pass . ",
    "url": "/hw4/#part-01-calibrating-your-camera",
    
    "relUrl": "/hw4/#part-01-calibrating-your-camera"
  },"30": {
    "doc": "Homework 4",
    "title": "Part 0.2: Capturing a 3D Object Scan",
    "content": "Next, pick a favorite object of yours, and print out a single ArUco tag, which you can generate from here. Place the object next to the tag on a tabletop, and capture 30-50 images of the object from different angles. Important: Use the same camera and zoom level as you did for calibration. If you cut out the ArUco tag from a piece of paper, make sure to leave a white border around it, otherwise detection will fail. Capture tips for good quality NeRF results later: . | Avoid brightness/exposure changes (this is why printing the tag will work better than using a tablet display) | Avoid blurry images, try not to introduce motion blur | Capture images at varying angles horizontally and vertically | Capture images at one uniform distance, about 10-20cm away from the object so that it fills ~50% of the frame | . ",
    "url": "/hw4/#part-02-capturing-a-3d-object-scan",
    
    "relUrl": "/hw4/#part-02-capturing-a-3d-object-scan"
  },"31": {
    "doc": "Homework 4",
    "title": "Part 0.3: Estimating Camera Pose",
    "content": "Now that you have calibrated your camera, you can use those intrinsic parameters to estimate the camera pose (position and orientation) for each image of your object. As discussed in lecture, this is the classic Perspective-n-Point (PnP) problem: given a set of 3D points in world coordinates and their corresponding 2D projections in an image, we want to find the camera’s extrinsic parameters (rotation and translation). For each image in your object scan, you’ll detect the single ArUco tag and use cv2.solvePnP() to estimate the camera pose. Here’s what you need: . Inputs to solvePnP: . | objectPoints (numpy array of shape (N, 3) or (N, 1, 3)): The 3D coordinates of the ArUco tag corners in world space. Since you know the physical size of your printed tag, you can define these coordinates. For example, if your tag is 0.02m × 0.02m, you might define the 4 corners as: [(0,0,0), (0.02,0,0), (0.02,0.02,0), (0,0.02,0)] in meters, with the tag lying flat on the z=0 plane. | imagePoints (numpy array of shape (N, 2) or (N, 1, 2)): The detected 2D pixel coordinates of the tag corners in the image (from detectMarkers()). You’ll need to reshape the corners array from detectMarkers to match this shape. | cameraMatrix (numpy array of shape (3, 3)): The intrinsic matrix K that you computed in Part 0.1 (contains focal length and principal point) | distCoeffs (numpy array or None): The distortion coefficients from calibration | . Output from solvePnP: . | success (bool): Whether the pose estimation succeeded | rvec (numpy array of shape (3, 1)): Axis-Angle rotation vector (can be converted to a 3×3 rotation matrix using cv2.Rodrigues()) | tvec (numpy array of shape (3, 1)): Translation vector | . Together, the rotation matrix R (from rvec) and translation vector tvec form the camera’s extrinsic matrix, which describes where the camera is positioned and oriented relative to the ArUco tag’s coordinate system (which we’re treating as the world origin). Note: OpenCV’s solvePnP() returns the world-to-camera transformation; you will need to invert this to get the camera-to-world (c2w) matrix for visualization and Part 0.4. Important: Just like in Part 0.1, make sure your code handles cases where the tag isn’t detected in some images. You should skip those images rather than letting your code crash. To help visualize your pose estimation results, you can use the following code snippet to display a camera frustum in 3D (pip install viser): . import viser import numpy as np server = viser.ViserServer(share=True) # Example of visualizing a camera frustum (in practice loop over all images) # c2w is the camera-to-world transformation matrix (3x4), and K is the camera intrinsic matrix (3x3) server.scene.add_camera_frustum( f\"/cameras/{i}\", # give it a name fov=2 * np.arctan2(H / 2, K[0, 0]), # field of view aspect=W / H, # aspect ratio scale=0.02, # scale of the camera frustum change if too small/big wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz, # orientation in quaternion format position=c2w[:3, 3], # position of the camera image=img # image to visualize ) while True: time.sleep(0.1) # Wait to allow visualization to run . [Deliverables] Include 2 screenshots of your cloud of cameras in Viser showing the camera frustums’ poses and images. ",
    "url": "/hw4/#part-03-estimating-camera-pose",
    
    "relUrl": "/hw4/#part-03-estimating-camera-pose"
  },"32": {
    "doc": "Homework 4",
    "title": "Part 0.4: Undistorting images and creating a dataset",
    "content": "Now that you have the camera intrinsics and pose estimates, the final step is to undistort your images and package everything into a dataset format that you can use for training NeRF in the later parts of this project. First, use cv2.undistort() to remove any lens distortion from your images. This is important because NeRF assumes a perfect pinhole camera model without distortion. import cv2 # Undistort an image using the calibration results undistorted_img = cv2.undistort(img, camera_matrix, dist_coeffs) . Note on black boundaries: If you see black boundaries after undistorting your images, you can use cv2.getOptimalNewCameraMatrix() to compute a new camera matrix that crops out the invalid pixels. This function returns both a new intrinsics matrix and a valid pixel ROI (region of interest). You can then crop your undistorted images to this ROI to remove the black borders. Important: If you crop your images this way, make sure to update the principal point in your intrinsics matrix to account for the crop offset, as the coordinate system origin has shifted. import cv2 # Example: Handling black boundaries from undistortion h, w = img.shape[:2] # alpha=1 keeps all pixels (more black borders), alpha=0 crops maximally new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix( camera_matrix, dist_coeffs, (w, h), alpha=0, (w, h) ) undistorted_img = cv2.undistort(img, camera_matrix, dist_coeffs, None, new_camera_matrix) # Crop to the valid region of interest x, y, w_roi, h_roi = roi undistorted_img = undistorted_img[y:y+h_roi, x:x+w_roi] # Update the principal point to account for the crop offset new_camera_matrix[0, 2] -= x # cx new_camera_matrix[1, 2] -= y # cy # Use new_camera_matrix (with updated principal point) when creating your dataset! . After undistorting all your images, you’ll need to save everything in a .npz file format that matches what’s expected for the NeRF training code. You should split your images into training, validation, and test sets, then save using the following keys: . | images_train: numpy array of shape (N_train, H, W, 3) containing your undistorted training images (0-255 range, will be normalized when loaded) | c2ws_train: numpy array of shape (N_train, 4, 4) containing the camera-to-world transformation matrices for training images | images_val: numpy array of shape (N_val, H, W, 3) for validation images | c2ws_val: numpy array of shape (N_val, 4, 4) for validation camera poses | c2ws_test: numpy array of shape (N_test, 4, 4) for test camera poses (used for novel view rendering) | focal: float representing the focal length from your camera intrinsics (assuming fx = fy) | . You can save your dataset using np.savez(): . import numpy as np # Package your data (keep images in 0-255 range, they'll be normalized when loaded) np.savez( 'my_data.npz', images_train=images_train, # (N_train, H, W, 3) c2ws_train=c2ws_train, # (N_train, 4, 4) images_val=images_val, # (N_val, H, W, 3) c2ws_val=c2ws_val, # (N_val, 4, 4) c2ws_test=c2ws_test, # (N_test, 4, 4) focal=focal # float ) . This dataset can then be loaded in Parts 1 and 2 the same way the provided lego dataset is loaded, allowing you to train a NeRF on your own captured object! . As a sanity check you can test your calibration implementation on our calibration images and our Lafufu dataset which we used to train the NeRF example seen at the end of this project. ",
    "url": "/hw4/#part-04-undistorting-images-and-creating-a-dataset",
    
    "relUrl": "/hw4/#part-04-undistorting-images-and-creating-a-dataset"
  },"33": {
    "doc": "Homework 4",
    "title": "Part 1: Fit a Neural Field to a 2D Image",
    "content": "From the lecture we know that we can use a Neural Radiance Field (NeRF) (\\(F: \\{x, y, z, \\theta, \\phi\\} \\rightarrow \\{r, g, b, \\sigma\\}\\)) to represent a 3D space. But before jumping into 3D, let’s first get familar with NeRF (and PyTorch) using a 2D example. In fact, since there is no concept of radiance in 2D, the Neural Radiance Field falls back to just a Neural Field (\\(F: \\{u, v\\} \\rightarrow \\{r, g, b\\}\\)) in 2D, in which \\(\\{u, v\\}\\) is the pixel coordinate. In this section, we will create a neural field that can represent a 2D image and optimize that neural field to fit this image. You can start from this image, but feel free to try out any other images. [Impl: Network] You would need to create an Multilayer Perceptron (MLP) network with Sinusoidal Positional Encoding (PE) that takes in the 2-dim pixel coordinates, and output the 3-dim pixel colors. | Multilayer Perceptron (MLP): An MLP is simply a stack of non linear activations (e.g., torch.nn.ReLU() or torch.nn.Sigmoid()) and fully connected layers (torch.nn.Linear()). For this part, you can start from building an MLP with the structure shown in the image below. Note that you would need to have a Sigmoid layer at the end of the MLP to constrain the network output be in the range of (0, 1), as a valid pixel color (don’t forget to also normalize your image from [0, 255] to [0, 1] when you use it for supervision!). You can take a reference from this tutorial on how to create an MLP in PyTorch. | Sinusoidal Positional Encoding (PE): PE is an operation that you apply a series of sinusoidal functions to the input coordinates, to expand its dimensionality (See equation 4 from this paper for reference). Note we also additionally keep the original input in PE, so the complete formulation is \\(PE(x) = \\{x, \\sin(2^0\\pi x), \\cos(2^0\\pi x), \\sin(2^1\\pi x), \\cos(2^1\\pi x), ..., \\sin(2^{L-1}\\pi x), \\cos(2^{L-1}\\pi x)\\}\\) in which \\(L\\) is the highest frequency level. You can start from \\(L=10\\) that maps a 2 dimension coordinate to a 42 dimension vector. | . [Impl: Dataloader] If the image is with high resolution, it might be not feasible train the network with the all the pixels in every iteration due to the GPU memory limit. So you need to implement a dataloader that randomly sample \\(N\\) pixels at every iteration for training. The dataloader is expected to return both the \\(N\\times2\\) 2D coordinates and \\(N\\times3\\) colors of the pixels, which will serve as the input to your network, and the supervision target, respectively (essentially you have a batch size of \\(N\\)). You would want to normalize both the coordinates (x = x / image_width, y = y / image_height) and the colors (rgbs = rgbs / 255.0) to make them within the range of [0, 1]. [Impl: Loss Function, Optimizer, and Metric] Now that you have the network (MLP) and the dataloader, you need to define the loss function and the optimizer before you can start training your network. You will use mean squared error loss (MSE) (torch.nn.MSELoss) between the predicted color and the groundtruth color. Train your network using Adam (torch.optim.Adam) with a learning rate of 1e-2. Run the training loop for 1000 to 3000 iterations with a batch size of 10k. For the metric, MSE is a good one but it is more common to use Peak signal-to-noise ratio (PSNR) when it comes to measuring the reconstruction quality of a image. If the image is normalized to [0, 1], you can use the following equation to compute PSNR from MSE: \\(\\text{PSNR} = 10 \\cdot \\log_{10}\\left(\\frac{1}{\\text{MSE}}\\right)\\) . [Impl: Hyperparameter Tuning] Vary the layer width (channel size) and the max frequency \\(L\\) for the positional encoding. [Deliverables] As a reference, the above images show the process of optimizing the network to fit on this image. | Report your model architecture including number of layers, width, and learning rate. Feel free to add other details you think are important. | Show training progression (images at different iterations, similar to the above reference) on both the provided test image and one of your own images. | Show final results for 2 choices of max positional encoding frequency and 2 choices of width (a 2x2 grid of results). Try very low values for these hyperparameters to see how it affects the outputs. | Show the PSNR curve for training on one image of your choice. | . ",
    "url": "/hw4/#part-1-fit-a-neural-field-to-a-2d-image",
    
    "relUrl": "/hw4/#part-1-fit-a-neural-field-to-a-2d-image"
  },"34": {
    "doc": "Homework 4",
    "title": "Part 2: Fit a Neural Radiance Field from Multi-view Images",
    "content": "Now that we are familiar with using a neural field to represent a image, we can proceed to a more interesting task that using a neural radiance field to represent a 3D space, through inverse rendering from multi-view calibrated images. For this part we are going to use the Lego scene from the original NeRF paper, but with lower resolution images (200 x 200) and preprocessed cameras (downloaded from here). The following code can be used to parse the data. The figure on its right shows a plot of all the cameras, including training cameras in black, validation cameras in red, and test cameras in green. data = np.load(f\"lego_200x200.npz\") # Training images: [100, 200, 200, 3] images_train = data[\"images_train\"] / 255.0 # Cameras for the training images # (camera-to-world transformation matrix): [100, 4, 4] c2ws_train = data[\"c2ws_train\"] # Validation images: images_val = data[\"images_val\"] / 255.0 # Cameras for the validation images: [10, 4, 4] # (camera-to-world transformation matrix): [10, 200, 200, 3] c2ws_val = data[\"c2ws_val\"] # Test cameras for novel-view video rendering: # (camera-to-world transformation matrix): [60, 4, 4] c2ws_test = data[\"c2ws_test\"] # Camera focal length focal = data[\"focal\"] # float . ",
    "url": "/hw4/#part-2-fit-a-neural-radiance-field-from-multi-view-images",
    
    "relUrl": "/hw4/#part-2-fit-a-neural-radiance-field-from-multi-view-images"
  },"35": {
    "doc": "Homework 4",
    "title": "Part 2.1: Create Rays from Cameras",
    "content": "Camera to World Coordinate Conversion. The transformation between the world space \\(\\mathbf{X_w} = (x_w, y_w, z_w)\\) and the camera space \\(\\mathbf{X_c} = (x_c, y_c, z_c)\\) can be defined as a rotation matrix \\(\\mathbf{R}_{3 \\times 3}\\) and a translation vector \\(\\mathbf{t}\\): \\(\\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\mathbf{R}_{3\\times3} &amp; \\mathbf{t} \\\\ \\mathbf{0}_{1\\times3} &amp; 1 \\end{bmatrix} \\begin{bmatrix} x_w \\\\ y_w \\\\ z_w \\\\ 1 \\end{bmatrix}\\) in which \\(\\begin{bmatrix} \\mathbf{R}_{3\\times3} &amp; \\mathbf{t} \\\\ \\mathbf{0}_{1\\times3} &amp; 1 \\end{bmatrix}\\) is called world-to-camera (w2c) transformation matrix, or extrinsic matrix. The inverse of it is called camera-to-world (c2w) transformation matrix. [Impl] In this session you would need to implement a function x_w = transform(c2w, x_c) that transform a point from camera to the world space. You can verify your implementation by checking if the follow statement is always true: x == transform(c2w.inv(), transform(c2w, x)). Note you might want your implementation to support batched coordinates for later use. You can implement it with either numpy or torch. Pixel to Camera Coordinate Conversion. Consider a pinhole camera with focal length \\((f_x, f_y)\\) and principal point \\((o_x = \\text{image_width} / 2, o_y = \\text{image_height} / 2)\\), its intrinsic matrix \\(\\mathbf{K}\\) is defined as: \\(\\mathbf{K} = \\begin{bmatrix} f_x &amp; 0 &amp; o_x \\\\ 0 &amp; f_y &amp; o_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\) which can be used to project a 3D point \\((x_c, y_c, z_c)\\) in the camera coordinate system to a 2D location \\((u, v)\\) in pixel coordinate system : \\(s \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = \\mathbf{K} \\begin{bmatrix} x_c \\\\ y_c \\\\ z_c \\end{bmatrix}\\) in which \\(s=z_c\\) is the depth of this point along the optical axis. [Impl] In this session you would need to implement a function that invert the aforementioned process, which transform a point from the pixel coordinate system back to the camera coordinate system: x_c = pixel_to_camera(K, uv, s). Similar to the previous session, you might also want your implementation here to support batched coordinates for later use. You can implement it with either numpy or torch. Pixel to Ray. A ray can be defined by an origin vector \\(\\mathbf{r}_o \\in \\mathbb{R}^3\\) and a direction vector \\(\\mathbf{r}_d \\in \\mathbb{R}^3\\). In the case of a pinhole camera, we want to know the \\(\\{\\mathbf{r}_o, \\mathbf{r}_d\\}\\) for every pixel \\((u, v)\\). The origin \\(\\mathbf{r}_o\\) of those rays is easy to get because it is just the location of the camera in world coordinates. For a camera-to-world (c2w) transformation matrix \\(\\begin{bmatrix} \\mathbf{R}_{3\\times3} &amp; \\mathbf{t} \\\\ \\mathbf{0}_{1\\times3} &amp; 1 \\end{bmatrix}\\), the camera origin is simply the translation component: \\(\\mathbf{r}_o = \\mathbf{t}\\) To calculate the ray direction for pixel \\((u, v)\\), we can simply choose a point along this ray with depth equal to 1 (\\(s=1\\)) and find its coordinate in world space \\(\\mathbf{X_w} = (x_w, y_w, z_w)\\) using your previously implemented functions. Then the normalized ray direction can be computed by: \\(\\mathbf{r}_d = \\frac{\\mathbf{X_w} - \\mathbf{r}_o}{\\|\\mathbf{X_w} - \\mathbf{r}_o\\|_2}\\) . [Impl] In this section you will need to implement a function that converts a pixel coordinate to a ray with origin and normalized direction: ray_o, ray_d = pixel_to_ray(K, c2w, uv). You might find your previously implemented functions useful here. Similarly, you might also want your implementation to support batched coordinates. ",
    "url": "/hw4/#part-21-create-rays-from-cameras",
    
    "relUrl": "/hw4/#part-21-create-rays-from-cameras"
  },"36": {
    "doc": "Homework 4",
    "title": "Part 2.2: Sampling",
    "content": "[Impl: Sampling Rays from Images] In Part 1, we have done random sampling on a single image to get the pixel color and pixel coordinates. Here we can build on top of that, and with the camera intrinsics &amp; extrinsics, we would be able to convert the pixel coordinates into ray origins and directions. Make sure to account for the offset from image coordinate to pixel center (this can be done simply by adding .5 to your UV pixel coordinate grid)! Since we have multiple images now, we have two options of sampling rays. Say we want to sample N rays at every training iteration, option 1 is to first sample M images, and then sample N // M rays from every image. The other option is to flatten all pixels from all images and do a global sampling once to get N rays from all images. You can choose whichever way you do ray sampling. [Impl: Sampling Points along Rays.] After having rays, we also need to discretize each ray into samples that live in the 3D space. The simplest way is to uniformly create some samples along the ray (t = np.linspace(near, far, n_samples)). For the lego scene that we have, we can set near=2.0 and far=6.0. The actual 3D coordinates can be acquired by $\\mathbf{x} = \\mathbf{r}_o + t \\mathbf{r}_d$. However this would lead to a fixed set of 3D points, which could potentially lead to overfitting when we train the NeRF later on. On top of this, we want to introduce some small perturbation to the points only during training, so that every location along the ray would be touched upon during training. this can be achieved by something like t = t + (np.random.rand(t.shape) * t_width) where t is set to be the start of each interval. We recommend to set n_samples to 32 or 64 in this project. ",
    "url": "/hw4/#part-22-sampling",
    
    "relUrl": "/hw4/#part-22-sampling"
  },"37": {
    "doc": "Homework 4",
    "title": "Part 2.3: Putting the Dataloading All Together",
    "content": "Similar to Part 1, you would need to write a dataloader that randomly sample pixels from multiview images. What is different with Part 1, is that now you need to convert the pixel coordinates into rays in your dataloader, and return ray origin, ray direction and pixel colors from your dataloader. To verify if you have by far implement everything correctly, we here provide some visualization code to plot the cameras, rays, and samples in 3D. We additionally recommend you try this code with rays sampled only from one camera so you can make sure that all the rays stay within the camera frustum and eliminating the possibility of other smaller harder to catch bugs. import viser, time # pip install viser import numpy as np # --- You Need to Implement These ------ dataset = RaysData(images_train, K, c2ws_train) rays_o, rays_d, pixels = dataset.sample_rays(100) # Should expect (B, 3) points = sample_along_rays(rays_o, rays_d, perturb=True) H, W = images_train.shape[1:3] # --------------------------------------- server = viser.ViserServer(share=True) for i, (image, c2w) in enumerate(zip(images_train, c2ws_train)): server.add_camera_frustum( f\"/cameras/{i}\", fov=2 * np.arctan2(H / 2, K[0, 0]), aspect=W / H, scale=0.15, wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz, position=c2w[:3, 3], image=image ) for i, (o, d) in enumerate(zip(rays_o, rays_d)): server.add_spline_catmull_rom( f\"/rays/{i}\", positions=np.stack((o, o + d * 6.0)), ) server.add_point_cloud( f\"/samples\", colors=np.zeros_like(points).reshape(-1, 3), points=points.reshape(-1, 3), point_size=0.02, ) while True: time.sleep(0.1) # Wait to allow visualization to run . # Visualize Cameras, Rays and Samples import viser, time import numpy as np # --- You Need to Implement These ------ dataset = RaysData(images_train, K, c2ws_train) # This will check that your uvs aren't flipped uvs_start = 0 uvs_end = 40_000 sample_uvs = dataset.uvs[uvs_start:uvs_end] # These are integer coordinates of widths / heights (xy not yx) of all the pixels in an image # uvs are array of xy coordinates, so we need to index into the 0th image tensor with [0, height, width], so we need to index with uv[:,1] and then uv[:,0] assert np.all(images_train[0, sample_uvs[:,1], sample_uvs[:,0]] == dataset.pixels[uvs_start:uvs_end]) # # Uncoment this to display random rays from the first image # indices = np.random.randint(low=0, high=40_000, size=100) # # Uncomment this to display random rays from the top left corner of the image # indices_x = np.random.randint(low=100, high=200, size=100) # indices_y = np.random.randint(low=0, high=100, size=100) # indices = indices_x + (indices_y * 200) data = {\"rays_o\": dataset.rays_o[indices], \"rays_d\": dataset.rays_d[indices]} points = sample_along_rays(data[\"rays_o\"], data[\"rays_d\"], random=True) # --------------------------------------- server = viser.ViserServer(share=True) for i, (image, c2w) in enumerate(zip(images_train, c2ws_train)): server.add_camera_frustum( f\"/cameras/{i}\", fov=2 * np.arctan2(H / 2, K[0, 0]), aspect=W / H, scale=0.15, wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz, position=c2w[:3, 3], image=image ) for i, (o, d) in enumerate(zip(data[\"rays_o\"], data[\"rays_d\"])): positions = np.stack((o, o + d * 6.0)) server.add_spline_catmull_rom( f\"/rays/{i}\", positions=positions, ) server.add_point_cloud( f\"/samples\", colors=np.zeros_like(points).reshape(-1, 3), points=points.reshape(-1, 3), point_size=0.03, ) while True: time.sleep(0.1) # Wait to allow visualization to run . . ",
    "url": "/hw4/#part-23-putting-the-dataloading-all-together",
    
    "relUrl": "/hw4/#part-23-putting-the-dataloading-all-together"
  },"38": {
    "doc": "Homework 4",
    "title": "Part 2.4: Neural Radiance Field",
    "content": "[Impl: Network] After having samples in 3D, we want to use the network to predict the density and color for those samples in 3D. So you would create a MLP that is similar to Part 1, but with three changes: . | Input is now 3D world coordinates instead of 2D pixel coordinates, along side a 3D vector as the ray direction. And we are going to output not only the color, but also the density for the 3D points. In the radiance field, the color of each point depends on the view direction, so we are going to use the view direction as the condition when we predict colors. Note we use Sigmoid to constrain the output color within range (0, 1), and use ReLU to constrain the output density to be positive. The ray direction also needs to be encoded by positional encoding (PE) but can use less frequency (e.g., L=4) than the cooridnate PE (e.g., L=10). | Make the MLP deeper. We are now doing a more challenging task of optimizing a 3D representation instead of 2D. So we need a more powerful network. | Inject the input (after PE) to the middle of your MLP through concatenation. It’s a general trick for deep neural network, that is helpful for it to not forgetting about the input. | . Below is a structure of the network that you can start with: . ",
    "url": "/hw4/#part-24-neural-radiance-field",
    
    "relUrl": "/hw4/#part-24-neural-radiance-field"
  },"39": {
    "doc": "Homework 4",
    "title": "Part 2.5: Volume Rendering",
    "content": "The core volume rendering equation is as follows: . \\[C(\\mathbf{r})=\\int_{t_n}^{t_f} T(t) \\sigma(\\mathbf{r}(t)) \\mathbf{c}(\\mathbf{r}(t), \\mathbf{d}) dt, \\text{ where } T(t)=\\exp \\left(-\\int_{t_n}^t \\sigma(\\mathbf{r}(s)) ds\\right)\\] This fundamentally means that at every small step \\(dt\\) along the ray, we add the contribution of that small interval \\([t, t + dt]\\) to that final color, and we do the infinitely many additions of these infinitesimally small intervals with an integral. The discrete approximation (thus tractable to compute) of this equation can be stated as the following: \\(\\hat{C}(\\mathbf{r})=\\sum_{i=1}^N T_i\\left(1-\\exp \\left(-\\sigma_i \\delta_i\\right)\\right) \\mathbf{c}_i, \\text { where } T_i=\\exp \\left(-\\sum_{j=1}^{i-1} \\sigma_j \\delta_j\\right)\\) where \\(\\mathbf{c}_i\\) is the color obtained from our network at sample location \\(i\\), \\(T_i\\) is the probability of a ray not terminating before sample location \\(i\\), and \\(1 - e^{-\\sigma_i \\delta_i}\\) is the probability of terminating at sample location \\(i\\). If your volume rendering works, the following snippet of code should pass the assert statement: . import torch torch.manual_seed(42) sigmas = torch.rand((10, 64, 1)) rgbs = torch.rand((10, 64, 3)) step_size = (6.0 - 2.0) / 64 rendered_colors = volrend(sigmas, rgbs, step_size) correct = torch.tensor([ [0.5006, 0.3728, 0.4728], [0.4322, 0.3559, 0.4134], [0.4027, 0.4394, 0.4610], [0.4514, 0.3829, 0.4196], [0.4002, 0.4599, 0.4103], [0.4471, 0.4044, 0.4069], [0.4285, 0.4072, 0.3777], [0.4152, 0.4190, 0.4361], [0.4051, 0.3651, 0.3969], [0.3253, 0.3587, 0.4215] ]) assert torch.allclose(rendered_colors, correct, rtol=1e-4, atol=1e-4) . [Impl] Here you will implement the volume rendering equation for a batch of samples along a ray. This rendered color is what we will compare with our posed images in order to train our network. You would need to implement this part in torch instead of numpy because we need the loss to be able to backpropagate through this part. A hint is that you may find torch.cumsum or torch.cumprod useful here. [Deliverables] As a reference, the images below show the process of optimizing the network to fit on our lego multi-view images from a novel view. The staff solution reaches above 23 PSNR with 1000 gradient steps and a batchsize of 10K rays per gradent step. The staff solution uses an Adam optimizer with a learning rate of 5e-4. For guaranteed full credit, achieve 23 PSNR for any number of iterations. | Include a brief description of how you implement each part. | Report the visualization of the rays and samples you draw at a single training step (along with the cameras), similar to the plot we show above. Plot up to 100 rays to make it less crowded. | Visualize the training process by plotting the predicted images across iterations, similar to the above reference, as well as the PSNR curve on the validation set (6 images). | After you train the network, you can use it to render a novel view image of the lego from arbitrary camera extrinsic. Show a spherical rendering of the lego video using the provided cameras extrinsics (c2ws_test in the npz file). You should be get a video like this (left is 10 after minutes training, right is 2.5 hrs training): | . ",
    "url": "/hw4/#part-25-volume-rendering",
    
    "relUrl": "/hw4/#part-25-volume-rendering"
  },"40": {
    "doc": "Homework 4",
    "title": "Part 2.6: Training with your own data",
    "content": "You will now use the dataset you created in part 0 to create a NeRF of your chosen object. After training a NeRF on your dataset render a gif of novel views from your scene. We have provided some starter code below which may be useful. [UPDATE 11/14/2025] . The calibrated Lafufu Dataset can be found here. def look_at_origin(pos): # Camera looks towards the origin forward = -pos / np.linalg.norm(pos) # Normalize the direction vector # Define up vector (assuming y-up) up = np.array([0, 1, 0]) # Compute right vector using cross product right = np.cross(up, forward) right = right / np.linalg.norm(right) # Recompute up vector to ensure orthogonality up = np.cross(forward, right) # Create the camera-to-world matrix c2w = np.eye(4) c2w[:3, 0] = right c2w[:3, 1] = up c2w[:3, 2] = forward c2w[:3, 3] = pos return c2w def rot_x(phi): return np.array([ [math.cos(phi), -math.sin(phi), 0, 0], [math.sin(phi), math.cos(phi), 0, 0], [0,0,1,0], [0,0,0,1], ]) # TODO: Change start position to a good position for your scene such as # the translation vector of one of your training camera extrinsics START_POS = np.array([1., 0., 0.]) NUM_SAMPLES = 60 frames = [] for phi in np.linspace(360., 0., NUM_SAMPLES, endpoint=False): c2w = look_at_origin(START_POS) extrinsic = rot_x(phi/180.*np.pi) @ c2w # Generate view for this camera pose # TODO: Add code for generating a view with your model from the current extrinsic frame = ... frames.append(frame) . Helpful Tips / Common Mistakes: . | When using the test data our near and far parameters are set to 2.0 and 6.0 respectively. You will likely have to adjust these for the real data you collect. These parameters represent the minimum and maximum distance away from the camera’s sensor that we start and stop sampling. For our example we found that near = 0.02 and far = 0.5 worked well, but you will likely have to do some experimenting to find values that work for you. | You might want to increase the number of samples along your rays for your real data. This will take longer to train, but can improve visual quality of your NeRF. For our implementation we first trained with 32 samples in order to ensure that there are no issues or bugs in other parts of our code and then increased to 64 samples per ray to get our final result. | If training is taking an unreasonable amount of time, your image resolution may be the issue. Attempting to train with too large of images may take a long time. If you resize your images you need to ensure that your intrinsics matrix reflects this change either by resizing before doing calibration or adjusting the intrinsics matrix after recovering it. | . [Impl] Train a NeRF on your chosen object dataset collected in part 0. Make sure to save the training loss over iterations as well as to generate intermediate renders for the deliverables. [Deliverables] Create a gif of a camera circling the object showing novel views and discuss any code or hyperparameter changes you had to make. Include a plot of the training loss as well as some intermediate renders of the scene while it is training. ",
    "url": "/hw4/#part-26-training-with-your-own-data",
    
    "relUrl": "/hw4/#part-26-training-with-your-own-data"
  },"41": {
    "doc": "Homework 4",
    "title": "Bells &amp; Whistles",
    "content": "Required for CS 280A students only: . | Render the depths map video for the Lego scene. Instead of compositing per-point colors to the pixel color in the volume rendering, we can also composite per-point depths to the pixel depth. (See the reference video below) | . Optional for all students: . The following are optional explorations for any students interested in going deeper with NeRF. | Better (more efficient) sampling: Implement course-to-fine PDF resampling as described in the original NeRF paper. | Better NeRF representations: Replace MLP with something more advanced to make it faster. (e.g. TensoRF or Instant-NGP). For this part it is ok to borrow some code from existing implementations (mark reference!) to your code base and see how it affect your NeRF optimization. | Improve PSNR to 30+: Aside from better sampling, better NeRF representations, try other things you can think of to improve the quality of the images to get 30+ db in PSNR. | Render the Lego video with a different background color than black. You would need to revisit the volume rendering equation to see where you should inject the background color. | Implement scene contraction for large scenes as specified in Mip-NeRF 360. This allows NeRF to handle unbounded scenes by contracting distant points into a bounded space. | Use nerfstudio to make a cool video! | . ",
    "url": "/hw4/#bells--whistles",
    
    "relUrl": "/hw4/#bells--whistles"
  },"42": {
    "doc": "Homework 4",
    "title": "Deliverables Checklist",
    "content": "Make sure your submission includes all of the following: . | Submit your webpage public URL to the class gallery by filling out this form. | . ",
    "url": "/hw4/#deliverables-checklist",
    
    "relUrl": "/hw4/#deliverables-checklist"
  },"43": {
    "doc": "Homework 4",
    "title": "Part 0: Camera Calibration and 3D Scanning",
    "content": ". | 2 screenshots of your camera frustums visualization in Viser | . ",
    "url": "/hw4/#part-0-camera-calibration-and-3d-scanning",
    
    "relUrl": "/hw4/#part-0-camera-calibration-and-3d-scanning"
  },"44": {
    "doc": "Homework 4",
    "title": "Part 1: Fit a Neural Field to a 2D Image",
    "content": ". | Model architecture report (number of layers, width, learning rate, and other important details) | Training progression visualization on both the provided test image and one of your own images | Final results for 2 choices of max positional encoding frequency and 2 choices of width (2x2 grid) | PSNR curve for training on one image of your choice | . ",
    "url": "/hw4/#part-1-fit-a-neural-field-to-a-2d-image-1",
    
    "relUrl": "/hw4/#part-1-fit-a-neural-field-to-a-2d-image-1"
  },"45": {
    "doc": "Homework 4",
    "title": "Part 2: Fit a Neural Radiance Field from Multi-view Images",
    "content": ". | Brief description of how you implemented each part | Visualization of rays and samples with cameras (up to 100 rays) | Training progression visualization with predicted images across iterations | PSNR curve on the validation set | Spherical rendering video of the Lego using provided test cameras | . ",
    "url": "/hw4/#part-2-fit-a-neural-radiance-field-from-multi-view-images-1",
    
    "relUrl": "/hw4/#part-2-fit-a-neural-radiance-field-from-multi-view-images-1"
  },"46": {
    "doc": "Homework 4",
    "title": "Part 2.6: Training with Your Own Data",
    "content": ". | GIF of camera circling your object showing novel views | Discussion of code or hyperparameter changes you made | Plot of training loss over iterations | Intermediate renders of the scene during training | . ",
    "url": "/hw4/#part-26-training-with-your-own-data-1",
    
    "relUrl": "/hw4/#part-26-training-with-your-own-data-1"
  },"47": {
    "doc": "Homework 4",
    "title": "Bells &amp; Whistles (if applicable)",
    "content": ". | CS 280A students: Depth map video for the Lego scene | Optional: Any additional explorations you completed | . ",
    "url": "/hw4/#bells--whistles-if-applicable",
    
    "relUrl": "/hw4/#bells--whistles-if-applicable"
  },"48": {
    "doc": "Homework 4",
    "title": "Homework 4",
    "content": " ",
    "url": "/hw4/",
    
    "relUrl": "/hw4/"
  },"49": {
    "doc": "Homework 5",
    "title": "Part A: The Power of Diffusion Models!",
    "content": " ",
    "url": "/hw5/#part-a-the-power-of-diffusion-models",
    
    "relUrl": "/hw5/#part-a-the-power-of-diffusion-models"
  },"50": {
    "doc": "Homework 5",
    "title": "\nDue: TBD\n",
    "content": "We recommend using GPUs from Colab to finish this project! . ",
    "url": "/hw5/",
    
    "relUrl": "/hw5/"
  },"51": {
    "doc": "Homework 5",
    "title": "Overview",
    "content": "In part A you will play around with diffusion models, implement diffusion sampling loops, and use them for other tasks such as inpainting and creating optical illusions. Instructions can be found below and in the provided notebook. Because part A is simply to get your feet wet with pre-trained diffusion models, all deliverables should be completed in the notebook. You will still submit a webpage with your results. START EARLY! . This project, in many ways, will be the most difficult project this semester. ",
    "url": "/hw5/#overview",
    
    "relUrl": "/hw5/#overview"
  },"52": {
    "doc": "Homework 5",
    "title": "Part 0: Setup",
    "content": "Gaining Access to DeepFloyd . We are going to use the DeepFloyd IF diffusion model. DeepFloyd is a two stage model trained by Stability AI. The first stage produces images of size $64 \\times 64$ and the second stage takes the outputs of the first stage and generates images of size $256 \\times 256$. We provide upsampling code at the very end of the notebook, though this is not required in your submission. Before using DeepFloyd, you must accept its usage conditions. To do so: . | Make a Hugging Face account and log in. | Accept the license on the model card of DeepFloyd/IF-I-XL-v1.0. For affiliation, you can fill in \"The University of California, Berkeley.\" Accepting the license on the stage I model card will auto accept for the other IF models. | Log in locally by entering your Hugging Face Hub access token below. You should be able to find and create tokens here. A read token is enough for this project. | . Play with the Model using Your Own Text Prompts! . DeepFloyd was trained as a text-to-image model, which takes text prompts as input and outputs images that are aligned with the text. However, a raw text string cannot be directly used as the model's input — we first need to convert it into a high-dimensional vector (of 4096 dimensions in our case) that the model can understand, a.k.a. prompt embeddings. Since prompt encoders are always very big and hard to run in your notebook, we provide two Huggingface clusters A and B for generating your own prompt embeddings! Both are the same and feel free to use either of them. Please follow their instructions to create a dictionary of embeddings for your prompts, download the resulting .pth file, and load it in Google Colab. Please note that both clusters have daily usage limits, so if you're unable to use one, please try another or try again tomorrow. Alternatively, START EARLY and download the .pth file in advance — you only need to generate it once, and you can reuse the downloaded file afterward. If the official site experiences issues or runs out of computation, you can download one of our precomputed embeddings, but this is a predefined set of prompts and lacks flexibility. We want to see your creativity! . Deliverables . | Come up with some interesting text prompts and generate their embeddings. | Choose 3 of your prompts to generate images and display the caption and the output of the model. Reflect on the quality of the outputs and their relationships to the text prompts. Make sure to try at least 2 different num_inference_steps values. | Report the random seed that you're using here. You should use the same seed all subsequent parts. | . Hints . | Since we ask you to generate visual anagrams and hybrid images, you may want to include several text pairs prompting them beforehand. | . ",
    "url": "/hw5/#part-0-setup",
    
    "relUrl": "/hw5/#part-0-setup"
  },"53": {
    "doc": "Homework 5",
    "title": "Part 1: Sampling Loops",
    "content": "In this part of the problem set, you will write your own “sampling loops” that use the pretrained DeepFloyd denoisers. These should produce high quality images such as the ones generated above. You will then modify these sampling loops to solve different tasks such as inpainting or producing optical illusions. Diffusion Models Primer . Starting with a clean image, $x_0$, we can iteratively add noise to an image, obtaining progressively more and more noisy versions of the image, $x_t$, until we're left with basically pure noise at timestep $t=T$. When $t=0$, we have a clean image, and for larger $t$ more noise is in the image. A diffusion model tries to reverse this process by denoising the image. By giving a diffusion model a noisy $x_t$ and the timestep $t$, the model predicts the noise in the image. With the predicted noise, we can either completely remove the noise from the image, to obtain an estimate of $x_0$, or we can remove just a portion of the noise, obtaining an estimate of $x_{t-1}$, with slightly less noise. To generate images from the diffusion model (sampling), we start with pure noise at timestep $T$ sampled from a gaussian distribution, which we denote $x_T$. We can then predict and remove part of the noise, giving us $x_{T-1}$. Repeating this process until we arrive at $x_0$ gives us a clean image. For the DeepFloyd models, $T = 1000$. The exact amount of noise added at each step is dictated by noise coefficients, $\\bar\\alpha_t$, which were chosen by the people who trained DeepFloyd. 1.1 Implementing the Forward Process . A key part of diffusion is the forward process, which takes a clean image and adds noise to it. In this part, we will write a function to implement this. The forward process is defined by: . $$q(x_t | x_0) = N(x_t ; \\sqrt{\\bar\\alpha} x_0, (1 - \\bar\\alpha_t)\\mathbf{I})\\tag{A.1}$$ . which is equivalent to computing $$ x_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1 - \\bar\\alpha_t} \\epsilon \\quad \\text{where}~ \\epsilon \\sim N(0, 1) \\tag{A.2}$$ That is, given a clean image $x_0$, we get a noisy image $ x_t $ at timestep $t$ by sampling from a Gaussian with mean $ \\sqrt{\\bar\\alpha_t} x_0 $ and variance $ (1 - \\bar\\alpha_t) $. Note that the forward process is not just adding noise -- we also scale the image. You will need to use the alphas_cumprod variable, which contains the $\\bar\\alpha_t$ for all $t \\in [0, 999]$. Remember that $t=0$ corresponds to a clean image, and larger $t$ corresponds to more noise. Thus, $\\bar\\alpha_t$ is close to 1 for small $t$, and close to 0 for large $t$. The test image of the Campanile can be downloaded at here, which you should then resize to 64x64. Run the forward process on the test image with $t \\in [250, 500, 750]$ and display the results. You should get progressively more noisy images. Deliverables . | Implement the noisy_im = forward(im, t) function | Show the Campanile at noise level [250, 500, 750]. | . Hints . | The torch.randn_like function is helpful for computing $\\epsilon$. | Use the alphas_cumprod variable, which contains an array of the hyperparameters, with alphas_cumprod[t] corresponding to $\\bar\\alpha_t$. | . Berkeley Campanile . Noisy Campanile at t=250 . Noisy Campanile at t=500 . Noisy Campanile at t=750 . 1.2 Classical Denoising . Let's try to denoise these images using classical methods. Again, take noisy images for timesteps [250, 500, 750], but use Gaussian blur filtering to try to remove the noise. Getting good results should be quite difficult, if not impossible. Deliverables . | For each of the 3 noisy Campanile images from the previous part, show your best Gaussian-denoised version side by side. | . Hint: . | torchvision.transforms.functional.gaussian_blur is useful. Here is the documentation. | . Noisy Campanile at t=250 . Noisy Campanile at t=500 . Noisy Campanile at t=750 . Gaussian Blur Denoising at t=250 . Gaussian Blur Denoising at t=500 . Gaussian Blur Denoising at t=750 . 1.3 One-Step Denoising . Now, we'll use a pretrained diffusion model to denoise. The actual denoiser can be found at stage_1.unet. This is a UNet that has already been trained on a very, very large dataset of $(x_0, x_t)$ pairs of images. We can use it to recover Gaussian noise from the image. Then, we can remove this noise to recover (something close to) the original image. Note: this UNet is conditioned on the amount of Gaussian noise by taking timestep $t$ as additional input. Because this diffusion model was trained with text conditioning, we also need a text prompt embedding. We provide the embedding for the prompt \"a high quality photo\" for you to use. Later on, you can use your own text prompts. Deliverables . | For the 3 noisy images from 1.2 (t = [250, 500, 750]): . | Use your forward function to add noise to your Campanile. | Estimate the noise in the new noisy image, by passing it through stage_1.unet | Remove the noise from the noisy image to obtain an estimate of the original image. | Visualize the original image, the noisy image, and the estimate of the original image | . | . Hints . | When removing the noise, you can't simply subtract the noise estimate. Recall that in equation A.2 we need to scale the noise. Look at equation A.2 to figure out how we predict $x_0$ from $x_t$ and $t$. | You will probably have to wrangle tensors to the correct device and into the correct data types. The functions .to(device) and .half() will be useful. The denoiser is loaded on the device cuda as half precision (to save memory), so inputs to the denoiser need to match them. | The signature for the unet is stage_1.unet(im_noisy, t, encoder_hidden_states=prompt_embeds, return_dict=False). You need to pass in the noisy image, the timestep, and the prompt embeddings. The return_dict argument just makes the output nicer. | The unet will output a tensor of shape (1, 6, 64, 64). This is because DeepFloyd was trained to predict the noise as well as variance of the noise. The first 3 channels is the noise estimate, which you will use. The second 3 channels is the variance estimate which you may ignore. | To save GPU memory, you should wrap all of your code in a with torch.no_grad(): context. This tells torch not to do automatic differentiation, and saves a considerable amount of memory. | . Noisy Campanile at t=250 . Noisy Campanile at t=500 . Noisy Campanile at t=750 . One-Step Denoised Campanile at t=250 . One-Step Denoised Campanile at t=500 . One-Step Denoised Campanile at t=750 . 1.4 Iterative Denoising . In part 1.3, you should see that the denoising UNet does a much better job of projecting the image onto the natural image manifold, but it does get worse as you add more noise. This makes sense, as the problem is much harder with more noise! . But diffusion models are designed to denoise iteratively. In this part we will implement this. In theory, we could start with noise $x_{1000}$ at timestep $T=1000$, denoise for one step to get an estimate of $x_{999}$, and carry on until we get $x_0$. But this would require running the diffusion model 1000 times, which is quite slow (and costs $$$). It turns out, we can actually speed things up by skipping steps. The rationale for why this is possible is due to a connection with differential equations. It's a tad complicated, and not within scope for this course, but if you're interested you can check out this excellent article. To skip steps we can create a new list of timesteps that we'll call strided_timesteps, which does just this. strided_timesteps[0] will correspond to the the largest $t$ (and thus the noisiest image) and strided_timesteps[-1] will correspond to $t = 0$ (and thus a clean image). One simple way of constructing this list is by introducing a regular stride step (e.g. stride of 30 works well). On the ith denoising step we are at $ t = $ strided_timesteps[i], and want to get to $ t' =$ strided_timesteps[i+1] (from more noisy to less noisy). To actually do this, we have the following formula: . \\[x_{t'} = \\frac{\\sqrt{\\bar\\alpha_{t'}}\\beta_t}{1 - \\bar\\alpha_t} x_0 + \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t'})}{1 - \\bar\\alpha_t} x_t + v_\\sigma\\tag{A.3}\\] where: . | $x_t$ is your image at timestep $t$ | $x_{t'}$ is your noisy image at timestep $t'$ where $t' &lt; t$ (less noisy) | $\\bar\\alpha_t$ is defined by alphas_cumprod, as explained above. | $\\alpha_t = \\frac{\\bar\\alpha_t}{\\bar\\alpha_{t'}}$ | $\\beta_t = 1 - \\alpha_t$ | $x_0$ is our current estimate of the clean image using one-step denoising | . The $v_\\sigma$ is random noise, which in the case of DeepFloyd is also predicted. The process to compute this is not very important, so we supply a function, add_variance, to do this for you. &lt;/p&gt; . You can think of this as a linear interpolation between the signal and noise: . Interpolation . See equations 6 and 7 of the DDPM paper for more information (Denoising Diffusion Probabilistic Models, the paper that introduces the diffusion model, which comes from Cal!). Be careful about bars above the alpha! Some have them and some do not. First, create the list strided_timesteps. You should start at timestep 990, and take step sizes of size 30 until you arrive at 0. After completing the problem set, feel free to try different \"schedules\" of timesteps. Also implement the function iterative_denoise(im_noisy, i_start), which takes a noisy image image, as well as a starting index i_start. The function should denoise an image starting at timestep timestep[i_start], applying the above formula to obtain an image at timestep t' = timestep[i_start + 1], and repeat iteratively until we arrive at a clean image. Add noise to the test image im to timestep timestep[10] and display this image. Then run the iterative_denoise function on the noisy image, with i_start = 10, to obtain a clean image and display it. Please display every 5th image of the denoising loop. Compare this to the \"one-step\" denoising method from the previous section, and to gaussian blurring. Deliverables . Using i_start = 10: . | Create strided_timesteps: a list of monotonically decreasing timesteps, starting at 990, with a stride of 30, eventually reaching 0. Also initialize the timesteps using the function stage_1.scheduler.set_timesteps(timesteps=strided_timesteps) | Complete the iterative_denoise function | Show the noisy Campanile every 5th loop of denoising (it should gradually become less noisy) | Show the final predicted clean image, using iterative denoising | Show the predicted clean image using only a single denoising step, as was done in the previous part. This should look much worse. | Show the predicted clean image using gaussian blurring, as was done in part 1.2. | . Hints . | Remember, the unet will output a tensor of shape (1, 6, 64, 64). This is because DeepFloyd was trained to predict the noise as well as variance of the noise. The first 3 channels is the noise estimate, which you will use here. The second 3 channels is the variance estimate which you will pass to the add_variance function | Read the documentation for the add_variance function to figure out how to use it to add the $v_\\sigma$ to the image. | Depending on if your final images are torch tensors or numpy arrays, you may need to modify the `show_images` call a bit. | . Noisy Campanile at t=90 . Noisy Campanile at t=240 . Noisy Campanile at t=390 . Noisy Campanile at t=540 . Noisy Campanile at t=690 . Original . Iteratively Denoised Campanile . One-Step Denoised Campanile . Gaussian Blurred Campanile . 1.5 Diffusion Model Sampling . In part 1.4, we use the diffusion model to denoise an image. Another thing we can do with the iterative_denoise function is to generate images from scratch. We can do this by setting i_start = 0 and passing im_noisy as random noise. This effectively denoises pure noise. Please do this, and show 5 results of the prompt\"a high quality photo\". Deliverables . | Show 5 sampled images. | . Hints . | Use torch.randn to make the noise. | Make sure you move the tensor to the correct device and correct data type by calling .half() and .to(device). | The quality of the images will not be spectacular, but should be reasonable images. We will fix this in the next section with CFG. | . Sample 1 . Sample 2 . Sample 3 . Sample 4 . Sample 5 . 1.6 Classifier-Free Guidance (CFG) . You may have noticed that the generated images in the prior section are not very good, and some are completely non-sensical. In order to greatly improve image quality (at the expense of image diversity), we can use a technicque called Classifier-Free Guidance. In CFG, we compute both a conditional and an unconditional noise estimate. We denote these $\\epsilon_c$ and $\\epsilon_u$. Then, we let our new noise estimate be: $$\\epsilon = \\epsilon_u + \\gamma (\\epsilon_c - \\epsilon_u) \\tag{A.4}$$ where $\\gamma$ controls the strength of CFG. Notice that for $\\gamma=0$, we get an unconditional noise estimate, and for $\\gamma=1$ we get the conditional noise estimate. The magic happens when $\\gamma &gt; 1$. In this case, we get much higher quality images. Why this happens is still up to vigorous debate. For more information on CFG, you can check out this blog post. Please implement the iterative_denoise_cfg function, identical to the iterative_denoise function but using classifier-free guidance. To get an unconditional noise estimate, we can just pass an empty prompt embedding to the diffusion model (the model was trained to predict an unconditional noise estimate when given an empty text prompt). Disclaimer Disclaimer Before, we used \"a high quality photo\" as a \"null\" condition. Now, we will use the actual \"\" null prompt for unconditional guidance for CFG. In the later part, you should always use \"\" null prompt for unconditional guidance. Deliverables . | Implement the iterative_denoise_cfg function | Show 5 images of \"a high quality photo\" with a CFG scale of $\\gamma=7$. Now this prompt becomes a condition (but fairly weak) to generate conditional noise! You will use your customized prompts as stronger conditions in part 1.7 - part 1.9. | . Hints . | You will need to run the UNet twice, once for the conditional prompt embedding, and once for the unconditional | The UNet will predict both a conditional and an unconditional variance. Just use the conditional variance with the add_variance function. | The resulting images should be much better than those in the prior section. | . Sample 1 with CFG . Sample 2 with CFG . Sample 3 with CFG . Sample 4 with CFG . Sample 5 with CFG . 1.7 Image-to-image Translation . Note: You should use CFG from this point forward. In part 1.4, we take a real image, add noise to it, and then denoise. This effectively allows us to make edits to existing images. The more noise we add, the larger the edit will be. This works because in order to denoise an image, the diffusion model must to some extent \"hallucinate\" new things -- the model has to be \"creative.\" Another way to think about it is that the denoising process \"forces\" a noisy image back onto the manifold of natural images. Here, we're going to take the original Campanile image, noise it a little, and force it back onto the image manifold without any conditioning. Effectively, we're going to get an image that is similar to the Campanile (with a low-enough noise level). This follows the SDEdit algorithm. To start, please run the forward process to get a noisy Campanile, and then run the iterative_denoise_cfg function using a starting index of [1, 3, 5, 7, 10, 20] steps and show the results, labeled with the starting index. You should see a series of \"edits\" to the original image, gradually matching the original image closer and closer. Deliverables . | Edits of the Campanile image, using the given prompt at noise levels [1, 3, 5, 7, 10, 20] with the conditional text prompt \"a high quality photo\" | Edits of 2 of your own test images, using the same procedure. | . Hints . | You should have a range of images, gradually looking more like the original image | . SDEdit with i_start=1 . SDEdit with i_start=3 . SDEdit with i_start=5 . SDEdit with i_start=7 . SDEdit with i_start=10 . SDEdit with i_start=20 . Campanile . 1.7.1 Editing Hand-Drawn and Web Images . This procedure works particularly well if we start with a nonrealistic image (e.g. painting, a sketch, some scribbles) and project it onto the natural image manifold. Please experiment by starting with hand-drawn or other non-realistic images and see how you can get them onto the natural image manifold in fun ways. We provide you with 2 ways to provide inputs to the model: . | Download images from the web | Draw your own images | . Please find an image from the internet and apply edits exactly as above. And also draw your own images, and apply edits exactly as above. Feel free to copy the prior cell here. For drawing inspiration, you can check out the examples on this project page. Deliverables . | 1 image from the web of your choice, edited using the above method for noise levels [1, 3, 5, 7, 10, 20] (and whatever additional noise levels you want) | 2 hand drawn images, edited using the above method for noise levels [1, 3, 5, 7, 10, 20] (and whatever additional noise levels you want) | . Hints . | We provide you with preprocessing code to convert web images to the format expected by DeepFloyd | Unfortunately, the drawing interface is hardcoded to be 300x600 pixels, but we need a square image. The code will center crop, so just draw in the middle of the canvas. | . Avocado at i_start=1 . Avocado at i_start=3 . Avocado at i_start=5 . Avocado at i_start=7 . Avocado at i_start=10 . Avocado at i_start=20 . Avocado . House at i_start=1 . House at i_start=3 . House at i_start=5 . House at i_start=7 . House at i_start=10 . House at i_start=20 . Original House Sketch . 1.7.2 Inpainting . We can use the same procedure to implement inpainting (following the RePaint paper). That is, given an image $x_{orig}$, and a binary mask $\\bf m$, we can create a new image that has the same content where $\\bf m$ is 0, but new content wherever $\\bf m$ is 1. To do this, we can run the diffusion denoising loop. But at every step, after obtaining $x_t$, we \"force\" $x_t$ to have the same pixels as $x_{orig}$ where $\\bf m$ is 0, i.e.: . $$ x_t \\leftarrow \\textbf{m} x_t + (1 - \\textbf{m}) \\text{forward}(x_{orig}, t) \\tag{A.5}$$ . Essentially, we leave everything inside the edit mask alone, but we replace everything outside the edit mask with our original image -- with the correct amount of noise added for timestep $t$. Please implement this below, and edit the picture to inpaint the top of the Campanile. Deliverables . | A properly implemented inpaint function | The Campanile inpainted (feel free to use your own mask) | 2 of your own images edited (come up with your own mask) . | look at the results from this paper for inspiration | . | . Hints . | Reuse the forward function you implemented earlier to implement inpainting | Because we are using the diffusion model for tasks it was not trained for, you may have to run the sampling process a few times before you get a nice result. | You can copy and paste your iterative_denoise_cfg function. To get inpainting to work should only require (roughly) 1-2 additional lines and a few small changes. | . Campanile . Mask . Hole to Fill . Campanile Inpainted . 1.7.3 Text-Conditional Image-to-image Translation . Now, we will do the same thing as SDEdit, but guide the projection with a text prompt. This is no longer pure \"projection to the natural image manifold\" but also adds control using language. This is simply a matter of changing the prompt from \"a high quality photo\" to any of your prompt! . Deliverables . | Edits of the Campanile, using the given prompt at noise levels [1, 3, 5, 7, 10, 20] | Edits of 2 of your own test images, using the same procedure | . Hints&lt;/b&gt; . | The images should gradually look more like original image, but also look like the text prompt. | . Rocket Ship at noise level 1 . Rocket Ship at noise level 3 . Rocket Ship at noise level 5 . Rocket Ship at noise level 7 . Rocket Ship at noise level 10 . Rocket Ship at noise level 20 . Campanile . 1.8 Visual Anagrams . In this part, we are finally ready to implement Visual Anagrams and create optical illusions with diffusion models. In this part, we will create an image that looks like \"an oil painting of people around a campfire\", but when flipped upside down will reveal \"an oil painting of an old man\". To do this, we will denoise an image $x_t$ at step $t$ normally with the prompt $p_1$, to obtain noise estimate $\\epsilon_1$. But at the same time, we will flip $x_t$ upside down, and denoise with the prompt $p_2$, to get noise estimate $\\epsilon_2$. We can flip $\\epsilon_2$ back, and average the two noise estimates. We can then perform a reverse/denoising diffusion step with the averaged noise estimate. The full algorithm will be: . $$ \\epsilon_1 = \\text{CFG of UNet}(x_t, t, p_1) $$ . $$ \\epsilon_2 = \\text{flip}(\\text{CFG of UNet}(\\text{flip}(x_t), t, p_2)) $$ . $$ \\epsilon = (\\epsilon_1 + \\epsilon_2) / 2 $$ . where UNet is the diffusion model UNet from before, $\\text{flip}(\\cdot)$ is a function that flips the image, and $p_1$ and $p_2$ are two different text prompt embeddings. And our final noise estimate is $\\epsilon$. Please implement the above algorithm and show example of an illusion. Deliverables . | Correctly implemented visual_anagrams function | 2 illusions of your choice that change appearance when you flip it upside down (feel free to take inspirations from this page). | . Hints . | You may have to run multiple times to get a really good result for the same reasons as above. | . An Oil Painting of an Old Man . An Oil Painting of People around a Campfire . 1.9 Hybrid Images . In this part we'll implement Factorized Diffusion and create hybrid images just like in project 2. In order to create hybrid images with a diffusion model we can use a similar technique as above. We will create a composite noise estimate $\\epsilon$, by estimating the noise with two different text prompts, and then combining low frequencies from one noise estimate with high frequencies of the other. The algorithm is: . $ \\epsilon_1 = \\text{CFG of UNet}(x_t, t, p_1) $ . $ \\epsilon_2 = \\text{CFG of UNet}(x_t, t, p_2) $ . $ \\epsilon = f_\\text{lowpass}(\\epsilon_1) + f_\\text{highpass}(\\epsilon_2)$ . where UNet is the diffusion model UNet, $f_\\text{lowpass}$ is a low pass function, $f_\\text{highpass}$ is a high pass function, and $p_1$ and $p_2$ are two different text prompt embeddings. Our final noise estimate is $\\epsilon$. Please show an example of a hybrid image using this technique (you may have to run multiple times to get a really good result for the same reasons as above). We recommend that you use a gaussian blur of kernel size 33 and sigma 2. Deliverables . | Correctly implemented make_hybrids function | 2 hybrid images of your choosing (feel free to take inspirations from this page). | . Hints . | use torchvision.transforms.functional.gaussian_blur | You may have to run multiple times to get a really good result for the same reasons as above | . Hybrid image of a skull and a waterfall . ",
    "url": "/hw5/#part-1-sampling-loops",
    
    "relUrl": "/hw5/#part-1-sampling-loops"
  },"54": {
    "doc": "Homework 5",
    "title": "Part 2: Bells &amp; Whistles",
    "content": "Required for CS280A students only: . | More visual anagrams! Visual anagrams in part 1.8 are created by flipping images upside down. However, there are much more transformations that also create visual anagrams! Refer to this paper and select two more transformations to create visual anagrams. | Design a course logo! Doing text-conditioned image-to-image translation on UCB's logo or your drawing may be a good idea. | . Optional for all students: . | Your own ideas: Be creative! | . ",
    "url": "/hw5/#part-2-bells--whistles",
    
    "relUrl": "/hw5/#part-2-bells--whistles"
  },"55": {
    "doc": "Homework 5",
    "title": "Deliverable Checklist",
    "content": ". | Make sure that your website and submission include all the deliverables in each section above. | Submit your PDF and code to corresponding assignments on Gradescope. | The Google Form is not required for Part A; you only need to complete the Google Form after both parts are finished. | . ",
    "url": "/hw5/#deliverable-checklist",
    
    "relUrl": "/hw5/#deliverable-checklist"
  },"56": {
    "doc": "Homework 5",
    "title": "Flow Matching from Scratch!",
    "content": "For this part, you need to submit your code and website PDF, and also your web url to class gallery via this Google Form. ",
    "url": "/hw5/#flow-matching-from-scratch",
    
    "relUrl": "/hw5/#flow-matching-from-scratch"
  },"57": {
    "doc": "Homework 5",
    "title": "\nDue: 12/12/2025 11:59pm\n",
    "content": "We recommend using GPUs from Colab to finish this project! . ",
    "url": "/hw5/",
    
    "relUrl": "/hw5/"
  },"58": {
    "doc": "Homework 5",
    "title": "Overview",
    "content": "You will train your own flow matching model on MNIST. Starter code can be found in the provided notebook. START EARLY! . ",
    "url": "/hw5/#overview-1",
    
    "relUrl": "/hw5/#overview-1"
  },"59": {
    "doc": "Homework 5",
    "title": "Neural Network Resources",
    "content": "In this part, you will build and train a UNet, which is more complex than the MLP you implemented in the NeRF project. We provide all class definitions you may need in the notebook (but feel free to add or modify them as necessary). Instead of asking ChatGPT to write everything for you, please consult the following resources when you get stuck — they will help you understand how and why things work under the hood. | PyTorch Documentation — Conv2d, ConvTranspose2d, and AvgPool2d. | PyTorch Documentation — torchvision.datasets.MNIST, the dataset we’re going to use, and torch.utils.data.DataLoader, the off-the-shelf dataloader we can directly use. | PyTorch tutorial on how to train a classifier on the CIFAR10 dataset. The structure of your training code will be very similar to this one. | . ",
    "url": "/hw5/#neural-network-resources",
    
    "relUrl": "/hw5/#neural-network-resources"
  },"60": {
    "doc": "Homework 5",
    "title": "Part 1: Training a Single-Step Denoising UNet",
    "content": "Let's warmup by building a simple one-step denoiser. Given a noisy image $z$, we aim to train a denoiser $D_\\theta$ such that it maps $z$ to a clean image $x$. To do so, we can optimize over an L2 loss: $$L = \\mathbb{E}_{z,x} \\|D_{\\theta}(z) - x\\|^2 \\tag{B.1}$$ . ",
    "url": "/hw5/#part-1-training-a-single-step-denoising-unet",
    
    "relUrl": "/hw5/#part-1-training-a-single-step-denoising-unet"
  },"61": {
    "doc": "Homework 5",
    "title": "1.1 Implementing the UNet",
    "content": "In this project, we implement the denoiser as a UNet. It consists of a few downsampling and upsampling blocks with skip connections. Figure 1: Unconditional UNet . The diagram above uses a number of standard tensor operations defined as follows: . Figure 2: Standard UNet Operations . &lt;br&gt; . where: . | Conv2d(kernel_size, stride, padding) is nn.Conv2d() | BN is nn.BatchNorm2d() | GELU is nn.GELU() | ConvTranspose2d(kernel_size, stride, padding) is nn.ConvTranspose2d() | AvgPool(kernel_size) is nn.AvgPool2d() | D is the number of hidden channels and is a hyperparameter that we will set ourselves. | . At a high level, the blocks do the following: . | (1) Conv is a convolutional layer that doesn't change the image resolution, only the channel dimension. | (2) DownConv is a convolutional layer that downsamples the tensor by 2. | (3) UpConv is a convolutional layer that upsamples the tensor by 2. | (4) Flatten is an average pooling layer that flattens a 7x7 tensor into a 1x1 tensor. 7 is the resulting height and width after the downsampling operations. | (5) Unflatten is a convolutional layer that unflattens/upsamples a 1x1 tensor into a 7x7 tensor. | (6) Concat is a channel-wise concatenation between tensors with the same 2D shape. This is simply torch.cat(). | . We define composed operations using our simple operations in order to make our network deeper. This doesn't change the tensor's height, width, or number of channels, but simply adds more learnable parameters. ",
    "url": "/hw5/#11-implementing-the-unet",
    
    "relUrl": "/hw5/#11-implementing-the-unet"
  },"62": {
    "doc": "Homework 5",
    "title": "1.2 Using the UNet to Train a Denoiser",
    "content": "Recall from equation 1 that we aim to solve the following denoising problem: . Given a noisy image $z$, we aim to train a denoiser $D_\\theta$ such that it maps $z$ to a clean image $x$. To do so, we can optimize over an L2 loss \\(L = \\mathbb{E}_{z,x} \\|D_{\\theta}(z) - x\\|^2.\\) . To train our denoiser, we need to generate training data pairs of ($z$, $x$), where each $x$ is a clean MNIST digit. For each training batch, we can generate $z$ from $x$ using the the following noising process: \\(z = x + \\sigma \\epsilon,\\quad \\text{where }\\epsilon \\sim N(0, I). \\tag{B.2}\\) . Visualize the different noising processes over $\\sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]$, assuming normalized $x \\in [0, 1]$. You should see noisier images as $\\sigma$ increases. Deliverable . | A visualization of the noising process using $\\sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]$. | . ",
    "url": "/hw5/#12-using-the-unet-to-train-a-denoiser",
    
    "relUrl": "/hw5/#12-using-the-unet-to-train-a-denoiser"
  },"63": {
    "doc": "Homework 5",
    "title": "1.2.1 Training",
    "content": "Now, we will train the model to perform denoising. | Objective: Train a denoiser to denoise noisy image $z$ with $\\sigma = 0.5$ applied to a clean image $x$. | Dataset and dataloader: Use the MNIST dataset via torchvision.datasets.MNIST。 Train only on the training set. Shuffle the dataset before creating the dataloader. Recommended batch size: 256. We'll train over our dataset for 5 epochs. | You should only noise the image batches when fetched from the dataloader so that in every epoch the network will see new noised images due to a random $\\epsilon$, improving generalization. | . | Model: Use the UNet architecture defined in section 1.1 with recommended hidden dimension D = 128. | Optimizer: Use Adam optimizer with learning rate of 1e-4. | . You should visualize denoised results on the test set at the end of training. Display sample results after the 1st and 5th epoch. &lt;/p&gt; . After 5 epoch training, they should look something like these: . Figure 3: Results on digits from the test set after 5 epochs of training . Deliverables . | A training loss curve plot every few iterations during the whole training process of $\\sigma = 0.5$. | Sample results on the test set with noise level 0.5 after the first and the 5-th epoch (staff solution takes ~3 minutes for 5 epochs on a Colab T4 GPU). | . ",
    "url": "/hw5/#121-training",
    
    "relUrl": "/hw5/#121-training"
  },"64": {
    "doc": "Homework 5",
    "title": "1.2.2 Out-of-Distribution Testing",
    "content": "Our denoiser was trained on MNIST digits noised with $\\sigma = 0.5$. Let's see how the denoiser performs on different $\\sigma$'s that it wasn't trained for. Visualize the denoiser results on test set digits with varying levels of noise $\\sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]$. Deliverables . | Sample results on the test set with out-of-distribution noise levels after the model is trained. Keep the same image and vary $\\sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]$. | . ",
    "url": "/hw5/#122-out-of-distribution-testing",
    
    "relUrl": "/hw5/#122-out-of-distribution-testing"
  },"65": {
    "doc": "Homework 5",
    "title": "1.2.3 Denoising Pure Noise",
    "content": "To make denoising a generative task, we'd like to be able to denoise pure, random Gaussian noise. We can think of this as starting with a blank canvas $z = \\epsilon$ where $\\epsilon \\sim N(0, I)$ and denoising it to get a clean image $x$. Repeat the same training process as in part 1.2.1, but input pure noise $\\epsilon \\sim N(0, I)$ and denoise it for 5 epochs. Display your results after 1 and 5 epochs. Sample from the denoiser that was trained to denoise pure noise. What patterns do you observe in the generated outputs? What relationship, if any, do these outputs have with the training images (e.g., digits 0–9)? Why might this be happening? . Deliverables . | A training loss curve plot every few iterations during the whole training process that denoises pure noise. | Sample results on pure noise after the first and the 5-th epoch. | A brief description of the patterns observed in the generated outputs and explanations for why they may exist. | . Hint . | For the last question, recall that with an MSE loss, the model learns to predict the point that minimizes the sum of squared distances to all training examples. This is closely related to the idea of a centroid in clustering. What does it represent in the context of the training images? | Since training can take a while, we strongly recommend that you checkpoint your model every epoch onto your personal Google Drive. This is because Colab notebooks aren't persistent such that if you are idle for a while, you will lose connection and your training progress. This consists of: . | Google Drive mounting. | Epoch-wise model &amp; optimizer checkpointing. | Model &amp; optimizer resuming from checkpoints. | . | . ",
    "url": "/hw5/#123-denoising-pure-noise",
    
    "relUrl": "/hw5/#123-denoising-pure-noise"
  },"66": {
    "doc": "Homework 5",
    "title": "Part 2: Training a Flow Matching Model",
    "content": "We just saw that one-step denoising does not work well for generative tasks. Instead, we need to iteratively denoise the image, and we will do so with flow matching. Here, we will iteratively denoise an image by training a UNet model to predict the `flow’ from our noisy data to clean data. In our flow matching setup, we sample a pure noise image $x_0 \\sim \\mathcal{N}(0, I)$ and generate a realistic image $x_1$. For iterative denoising, we need to define how intermediate noisy samples are constructed. The simplest approach would be a linear interpolation between noisy $x_0$ and clean $x_1$ for some $x_1$ in our training data: . \\begin{equation} x_t = (1-t)x_0 + tx_1 \\quad \\text{where } x_0 \\sim \\mathcal{N}(0, 1), t \\in [0, 1]. \\tag{B.3} \\end{equation} . This is a vector field describing the position of a point $x_t$ at time $t$ relative to the clean data distribution $p_1(x_1)$ and the noisy data distribution $p_0(x_0)$. Intuitively, we see that for small $t$, we remain close to noise, while for larger $t$, we approach the clean distribution. Flow can be thought of as the velocity (change in posiiton w.r.t. time) of this vector field, describing how to move from $x_0$ to $x_1$: \\begin{equation} u(x_t, t) = \\frac{d}{dt} x_t = x_1 - x_0. \\tag{B.4}\\end{equation} . Our aim is to learn a UNet $u_\\theta(x_t,t)$ which approximates this flow $u(x_t, t) = x_1 - x_0$, giving us our learning objective: \\begin{equation} L = \\mathbb{E}_{x_0 \\sim p_0(x_0), x_1 \\sim p_1(x_1), t \\sim U[0, 1]} \\|(x_1-x_0) - u_\\theta(x_t, t)\\|^2. \\tag{B.5} \\end{equation} . ",
    "url": "/hw5/#part-2-training-a-flow-matching-model",
    
    "relUrl": "/hw5/#part-2-training-a-flow-matching-model"
  },"67": {
    "doc": "Homework 5",
    "title": "2.1 Adding Time Conditioning to UNet",
    "content": "We need a way to inject scalar $t$ into our UNet model to condition it. There are many ways to do this. Here is what we suggest: . Figure 4: Conditioned UNet . Note: It may look like we're predicting the original image in the figure above, but we are not. We're predicting the flow from the noisy $x_0$ to clean $x_1$, which will contain both parts of the original image as well as the noise to remove. This uses a new operator called FCBlock (fully-connected block) which we use to inject the conditioning signal into the UNet: . Figure 5: FCBlock for conditioning . Here Linear(F_in, F_out) is a linear layer with F_in input features and F_out output features. You can implement it using nn.Linear. Since our conditioning signal $t$ is a scalar, F_in should be of size 1. You can embed $t$ by following this pseudo code: . fc1_t = FCBlock(...) fc2_t = FCBlock(...) # the t passed in here should be normalized to be in the range [0, 1] t1 = fc1_t(t) t2 = fc2_t(t) # Follow diagram to get unflatten. # Replace the original unflatten with modulated unflatten. unflatten = unflatten * t1 # Follow diagram to get up1... # Replace the original up1 with modulated up1. up1 = up1 * t2 # Follow diagram to get the output... ",
    "url": "/hw5/#21-adding-time-conditioning-to-unet",
    
    "relUrl": "/hw5/#21-adding-time-conditioning-to-unet"
  },"68": {
    "doc": "Homework 5",
    "title": "2.2 Training the UNet",
    "content": "Training our time-conditioned UNet $u_\\theta(x_t, t)$ is now pretty easy. Basically, we pick a random image $x_1$ from the training set, a random timestep $t$, add noise to $x_1$ to get $x_t$, and train the denoiser to predict the flow at $x_t$. We repeat this for different images and different timesteps until the model converges and we are happy. Algorithm B.1. Training time-conditioned UNet . | Objective: Train a time-conditioned UNet $u_\\theta(x_t, t)$ to predict the flow at $x_t$ given a noisy image $x_t$ and a timestep $t$. | Dataset and dataloader: Use the MNIST dataset via torchvision.datasets.MNIST. Train only on the training set. Shuffle the dataset before creating the dataloader. Recommended batch size: 64. | As shown in algorithm B.1, You should only noise the image batches when fetched from the dataloader. | . | Model: Use the time-conditioned UNet architecture defined in section 2.1 with recommended hidden dimension D = 64. Follow the diagram and pseudocode for how to inject the conditioning signal $t$ into the UNet. Remember to normalize $t$ before embedding it. | Optimizer: Use Adam optimizer with an initial learning rate of 1e-2. We will be using an exponential learning rate decay scheduler with a gamma of $0.1^{(1.0 / \\text{num_epochs})}$. This can be implemented using scheduler = torch.optim.lr_scheduler.ExponentialLR(...). You should call scheduler.step() after every epoch. | . Deliverable . | A training loss curve plot for the time-conditioned UNet over the whole training process. | . ",
    "url": "/hw5/#22-training-the-unet",
    
    "relUrl": "/hw5/#22-training-the-unet"
  },"69": {
    "doc": "Homework 5",
    "title": "2.3 Sampling from the UNet",
    "content": "We can now use our UNet for iterative denoising using the algorithm below! The results would not be perfect, but legible digits should emerge . Algorithm B.2. Sampling from time-conditioned UNet . Epoch 1 . Epoch 10 . Deliverables . | Sampling results from the time-conditioned UNet for 1, 5, and 10 epochs. The results should not be perfect, but reasonably good. | (Optional for CS180, required for CS280A) Check the Bells and Whistles if you want to make it better! | . ",
    "url": "/hw5/#23-sampling-from-the-unet",
    
    "relUrl": "/hw5/#23-sampling-from-the-unet"
  },"70": {
    "doc": "Homework 5",
    "title": "2.4 Adding Class-Conditioning to UNet",
    "content": "To make the results better and give us more control for image generation, we can also optionally condition our UNet on the class of the digit 0-9. This will require adding 2 more FCBlocks to our UNet but, we suggest that for class-conditioning vector $c$, you make it a one-hot vector instead of a single scalar. Because we still want our UNet to work without it being conditioned on the class (recall the classifer-free guidance you implemented in part a), we implement dropout where 10% of the time ($p_{\\text{uncond}}= 0.1$) we drop the class conditioning vector $c$ by setting it to 0. Here is one way to condition our UNet $u_\\theta(x_t, t, c)$ on both time $t$ and class $c$: . fc1_t = FCBlock(...) fc1_c = FCBlock(...) fc2_t = FCBlock(...) fc2_c = FCBlock(...) t1 = fc1_t(t) c1 = fc1_c(c) t2 = fc2_t(t) c2 = fc2_c(c) # Follow diagram to get unflatten. # Replace the original unflatten with modulated unflatten. unflatten = c1 * unflatten + t1 # Follow diagram to get up1... # Replace the original up1 with modulated up1. up1 = c2 * up1 + t2 # Follow diagram to get the output... ",
    "url": "/hw5/#24-adding-class-conditioning-to-unet",
    
    "relUrl": "/hw5/#24-adding-class-conditioning-to-unet"
  },"71": {
    "doc": "Homework 5",
    "title": "2.5 Training the UNet",
    "content": "Training for this section will be the same as time-only, with the only difference being the conditioning vector $c$ and doing unconditional generation periodically. Algorithm B.3. Training class-conditioned UNet . Deliverable . | A training loss curve plot for the class-conditioned UNet over the whole training process. | . ",
    "url": "/hw5/#25-training-the-unet",
    
    "relUrl": "/hw5/#25-training-the-unet"
  },"72": {
    "doc": "Homework 5",
    "title": "2.6 Sampling from the UNet",
    "content": "Now we will sample with class-conditioning and will use classifier-free guidance with $\\gamma = 5.0$. Algorithm B.4. Sampling from class-conditioned UNet . Epoch 1 . Epoch 10 . Deliverables . | Sampling results from the class-conditioned UNet for 1, 5, and 10 epochs. Class-conditioning lets us converge faster, hence why we only train for 10 epochs. Generate 4 instances of each digit as shown above. | Can we get rid of the annoying learning rate scheduler? Simplicity is the best. Please try to maintain the same performance after removing the exponential learning rate scheduler. Show your visualization after training without the scheduler and provide a description of what you did to compensate for the loss of the scheduler. | . ",
    "url": "/hw5/#26-sampling-from-the-unet",
    
    "relUrl": "/hw5/#26-sampling-from-the-unet"
  },"73": {
    "doc": "Homework 5",
    "title": "Part 3: Bells &amp; Whistles",
    "content": "Required for CS280A students only: . | A better time-conditioned only UNet: Our time-conditioning only UNet in part 2.3 is actually far from perfect. Its result is way worse than the UNet conditioned by both time and class. We can definitively make it better! Show a better visualization image for the time-conditioning only network. Possible approaches include extending the training schedule or making the architecture more expressive. | . Optional for all students: . | Your own ideas: Be creative! This UNet can generate images more than digits! You can try it on SVHN (still digits, but more fancy!), Fashion-MNIST (not digits, but still grayscale!), or CIFAR10! | . ",
    "url": "/hw5/#part-3-bells--whistles",
    
    "relUrl": "/hw5/#part-3-bells--whistles"
  },"74": {
    "doc": "Homework 5",
    "title": "Deliverable Checklist",
    "content": ". | Make sure that your website and submission include all the deliverables in each section above. | Submit your PDF and code to corresponding assignments on Gradescope. | The Google Form is required for Part B. Once you have finished both parts A and B, submit the link to your webpage (containing both parts) using this Google Form. | . Acknowledgements . This project was a joint effort by Ryan Tabrizi, Daniel Geng, Hang Gao, and Jingfeng Yang, advised by Liyue Shen, Andrew Owens, Angjoo Kanazawa, and Alexei Efros. We also thank David McAllister and Songwei Ge for their helpful feedback and suggestions. ",
    "url": "/hw5/#deliverable-checklist-1",
    
    "relUrl": "/hw5/#deliverable-checklist-1"
  },"75": {
    "doc": "Homework 5",
    "title": "Homework 5",
    "content": "| Hole Filling . \"Make it Real\" . A Lithograph of a Waterfall . A Lithograph of a Skull . An Oil Painting of an Old Man . An Oil Painting of People Around a Fire . ",
    "url": "/hw5/",
    
    "relUrl": "/hw5/"
  },"76": {
    "doc": "Home",
    "title": "COMS4732 - Computer Vision 2",
    "content": "Welcome to the course homepage. Select a homework assignment below: . ",
    "url": "/#coms4732---computer-vision-2",
    
    "relUrl": "/#coms4732---computer-vision-2"
  },"77": {
    "doc": "Home",
    "title": "Homework Assignments",
    "content": ". | Homework 1 | Homework 2 | Homework 3 | Homework 4 | Homework 5 | . ",
    "url": "/#homework-assignments",
    
    "relUrl": "/#homework-assignments"
  },"78": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  }
}
