<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(4)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(4) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(4) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(4) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Homework 3 | COMS4732W Computer Vision 2</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="Homework 3" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="A starter template for a Jeykll site using the Just the Docs theme!" /> <meta property="og:description" content="A starter template for a Jeykll site using the Just the Docs theme!" /> <link rel="canonical" href="http://localhost:4000/hw3/" /> <meta property="og:url" content="http://localhost:4000/hw3/" /> <meta property="og:site_name" content="COMS4732W Computer Vision 2" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Homework 3" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"A starter template for a Jeykll site using the Just the Docs theme!","headline":"Homework 3","url":"http://localhost:4000/hw3/"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <header class="side-bar"> <div class="site-header"> <a href="/" class="site-title lh-tight"> COMS4732W Computer Vision 2 </a> <button id="menu-button" class="site-button btn-reset" aria-label="Menu" aria-expanded="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="/hw1/" class="nav-list-link">Homework 1</a></li><li class="nav-list-item"><a href="/hw2/" class="nav-list-link">Homework 2</a></li><li class="nav-list-item"><a href="/hw3/" class="nav-list-link">Homework 3</a></li><li class="nav-list-item"><a href="/hw4/" class="nav-list-link">Homework 4</a></li><li class="nav-list-item"><a href="/hw5/" class="nav-list-link">Homework 5</a></li></ul> </nav> <div class="d-md-block d-xs-none"> <div class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </div> </div> </header> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search COMS4732W Computer Vision 2" autocomplete="off"> <label for="search-input" class="search-label"> <span class="sr-only">Search COMS4732W Computer Vision 2</span> <svg viewBox="0 0 24 24" class="search-icon" aria-hidden="true"><use xlink:href="#svg-search"></use></svg> </label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/just-the-docs/just-the-docs-template" class="site-button" > Template Repository </a> </li> </ul> </nav> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script> MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true } }; </script> <style> h1 { font-size: x-large; } h1 a { font-size: medium; } h1 img { float: left; padding-right: 1em; } h2 { font-size: x-large; text-align: left; font-variant: small-caps; } h2 b { font-size: large; font-variant: normal; color: red; } h2 i { font-size: large; font-variant: normal; font-style: italic; font-weight: normal; } h3 { font-size: large; font-variant: small-caps; margin: 1em 0 0 0; } /* Ensure h5 is at least as large as paragraph text */ h5 { font-size: 1em; } p { margin: 0 1em 0.5em 1em; } ul, ol { margin: 0.5em 0 0.5em 1em; } li { margin: 0; } /* Rubric layout styles (scoped to this page) */ .rubric { display: flex; justify-content: space-between; gap: 0.5rem; align-items: flex-start; flex-wrap: wrap; } .rubric-col { width: 48%; min-width: 320px; } .rubric-indent { margin-left: 0.5rem; } .rubric-note { font-size: 0.95em; } .rubric h5 { margin: 0.25em 0; } </style><header> <h1> Homework 3<br /> <a href="../../">COMS4732: Computer Vision 2</a> </h1> </header> <h2 style="text-align: center;"> <!-- <div style="display: flex; justify-content: center; gap: 1em; align-items: center; flex-wrap: wrap;"> <img src="/hws/hw2/image002.gif" alt="Feature Matching Example"> <img src="/hws/hw2/image003.gif" alt="Feature Matching Example 2"> </div><br> --> Simple Structure from Motion<br /> <b style="color:#9E0000">Due Date: TBD</b> </h2> <h1 id="background"> <a href="#background" class="anchor-heading" aria-labelledby="background"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Background </h1> <p>This assignment will involve determining the 3D position, or pose, of multiple cameras in a scene. We will build off of homework 2’s feature matching algorithm and modify RANSAC.</p> <p><br /> <strong>Important:</strong> this assignment largely depends on homework 2. Please make sure you have completed it before starting this assignment. <br /> <br /> <strong>Important:</strong> the overview isn’t exhaustive. You are expected to have attended lecture.</p> <h2 id="problem-statement"> <a href="#problem-statement" class="anchor-heading" aria-labelledby="problem-statement"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Problem statement </h2> <div style="text-align: center;"> <figure style="display: inline-block; margin: 1em 0;"> <img src="/hws/hw3/assets/problem_statement.png" alt="Problem statement diagram" style="max-width: 100%;" /> <figcaption style="margin-top: 0.5em; font-style: italic;"> <strong>Figure 1:</strong> Left: 3D problem space: you may or may not have 2D correspondences, 3D camera positions/motion, or 3D structure for a given 3D scene. <br /> Right: this homework will focus on when we know nothing about the scene. </figcaption> </figure> </div> <p>In the previous assignment, we computed correspondences between features in two images. The goal of this assignment is to similarly find correspondences between features in multiple images that will then be used to estimate the camera poses in the scene. Lastly, we will perform triangulation to also use the camera poses to estimate the 3D structure of the scene.</p> <h3 id="epipolar-geometry"> <a href="#epipolar-geometry" class="anchor-heading" aria-labelledby="epipolar-geometry"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Epipolar Geometry </h3> <h1 id="step-1-feature-extraction"> <a href="#step-1-feature-extraction" class="anchor-heading" aria-labelledby="step-1-feature-extraction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 1: Feature extraction </h1> <p>In HW2 we used Harris corners as a simple feature. Because the task of SfM involves finding correspondences between images whose cameras have been translated and rotated relative to each other, we need to use a more robust feature, particularly one that is invariant to rotation. We will use SIFT (Scale-Invariant Feature Transform) features.</p> <p>Here’s how to use SIFT in OpenCV:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cv2</span>

<span class="c1"># Create SIFT detector, detecting at most max_features features
</span><span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">SIFT_create</span><span class="p">(</span><span class="n">nfeatures</span><span class="o">=</span><span class="n">max_features</span><span class="p">)</span>

<span class="c1"># Detect keypoints and compute descriptors
</span><span class="n">keypoints</span><span class="p">,</span> <span class="n">descriptors</span> <span class="o">=</span> <span class="n">sift</span><span class="p">.</span><span class="nf">detectAndCompute</span><span class="p">(</span><span class="n">img_uint8</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

<span class="k">if</span> <span class="n">descriptors</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="nf">len</span><span class="p">(</span><span class="n">keypoints</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">WARNING: No SIFT features detected!</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[],</span> <span class="p">[]]),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([])</span>

<span class="c1"># Extract coordinates in (x, y) format from keypoints
</span><span class="n">coords_xy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">kp</span><span class="p">.</span><span class="n">pt</span> <span class="k">for</span> <span class="n">kp</span> <span class="ow">in</span> <span class="n">keypoints</span><span class="p">])</span>  <span class="c1"># (N, 2) in (x, y)
</span></code></pre></div></div> <p>Note: SIFT has keypoint selection baked in according to the <code class="language-plaintext highlighter-rouge">max_features</code> best keypoints. As such, we don’t need to perform ANMS here.</p> <h1 id="step-2-feature-matching"> <a href="#step-2-feature-matching" class="anchor-heading" aria-labelledby="step-2-feature-matching"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 2: Feature matching </h1> <p>Now, as we did in HW2, we need to find correspondences between features in multiple images. We will use the same approach of visualizing the nearest neighbor distance ratio (NNDR) and setting a threshold on which matched descriptors we’ll keep.</p> <p><strong>Note:</strong> With SIFT, L2 is typically used to measure the similarity between descriptors, whereas in HW2 we used NCC.</p> <h1 id="step-3-ransac-to-estimate-camera-pose"> <a href="#step-3-ransac-to-estimate-camera-pose" class="anchor-heading" aria-labelledby="step-3-ransac-to-estimate-camera-pose"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 3: RANSAC to estimate camera pose </h1> <div style="text-align: center;"> <figure style="display: inline-block; margin: 1em 0;"> <img src="/hws/hw3/assets/correspondences_to_pose.png" alt="Correspondences to pose diagram" style="max-width: 100%;" /> <figcaption style="margin-top: 0.5em; font-style: italic;"> <strong>Figure 2:</strong> Step 3 aims to recover the motion of both cameras, which is another way of saying the rotation $R$ and translation $t$ between our two cameras. </figcaption> </figure> </div> <p>Whereas in HW2 we used RANSAC to estimate the homography that relates two images, here we will use RANSAC to estimate the camera poses that relate two images. We do this via epipola geometry.</p> <div style="text-align: center;"> <figure style="display: inline-block; margin: 1em 0;"> <img src="/hws/hw3/assets/epipolar1.png" alt="Epipolar geometry setup" style="max-width: 100%;" /> <figcaption style="margin-top: 0.5em; font-style: italic;"> <strong>Figure 3:</strong> Epipolar geometry relates two camera views of the same 3D point. </figcaption> </figure> </div> <p>Suppose 2D points \(x\) and \(x'\) are the projections of a 3D point \(X\) onto the image planes of two cameras. The epipolar geometry relates these two points via the essential matrix \(E\): \(x'^T E x = 0\)</p> <div style="text-align: center;"> <figure style="display: inline-block; margin: 1em 0;"> <img src="/hws/hw3/assets/epipolar2.png" alt="Epipolar geometry setup" style="max-width: 100%;" /> <figcaption style="margin-top: 0.5em; font-style: italic;"> <strong>Figure 3:</strong> Constructing the epipolar constraint. </figcaption> </figure> </div> <p>To see how we get here, we first observe that $x’$ is related to $x$ by a rotation and translation:</p> \[x' = R x + t\] <p>Where $t$ is the translation from camera 1’s center of projection \(O\) to camera 2’s center of projection \(O'\). $R$ is the rotation from camera 1’s coordinate system to camera 2’s coordinate system.</p> <p>To relate the fact that $x$, $x’$, and $t$ are all coplanar, we define the normal vector to the plane as $n = t \times x’$ and substitute in the expression for $x’$:</p> \[\begin{aligned} n &amp;= t \times x' \\ n &amp;= t \times (R x + t) \\ n &amp;= t \times R x + \cancel{t \times t} \quad (\text{since } t \times t = 0)\\ n &amp;= t \times R x \end{aligned}\] <p>Lastly, by definition of $n$,</p> \[\begin{aligned} x' \cdot n &amp;= 0 \\ x' \cdot (t \times (R x)) &amp;= 0 \\ x'^T [t_{\leftrightarrow}] R x &amp;= 0 \\ x'^T E x &amp;= 0 \end{aligned}\] <p>Where \([t_{\leftrightarrow}]\) is a <a href="https://en.wikipedia.org/wiki/Skew-symmetric_matrix#Cross_product">skew-symmetric matrix representing the cross product</a> of $t$ with $R x$ and \(E = [t_{\leftrightarrow}] R\) is the essential matrix.</p> <p>Thus, for every $(x, x’)$ correspondence pair, we have an epipolar constraint relating the two by a rotation and translation: $x’^T E x = 0$.</p> <p>We can construct a system of equations for every $(x, x’)$ correspondence pair by unrolling E into a vector of variables:</p> \[x = (u, v, 1)^T, \quad x' = (u', v', 1)\] \[\begin{bmatrix} u' &amp; v' &amp; 1 \end{bmatrix} \begin{bmatrix} e_{11} &amp; e_{12} &amp; e_{13} \\ e_{21} &amp; e_{22} &amp; e_{23} \\ e_{31} &amp; e_{32} &amp; e_{33} \end{bmatrix} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = 0 \quad \Rightarrow \quad \begin{bmatrix} u'u &amp; u'v &amp; u' &amp; v'u &amp; v'v &amp; v' &amp; u &amp; v &amp; 1 \end{bmatrix} \begin{bmatrix} e_{11} \\ e_{12} \\ e_{13} \\ e_{21} \\ e_{22} \\ e_{23} \\ e_{31} \\ e_{32} \\ e_{33} \end{bmatrix} = 0\] <p>Each of these is a constraint in the system of equations:</p> \[A e = 0\] <p>The rotation \(R\) has 3 degrees of freedom (pitch, roll, yaw), and the translation \(t\) has 3 degrees of freedom (x, y, z). Thus, we have 6 degrees of freedom in total. Note, however, that in figure 3 the translation between \(O\) and \(O'\) is ambiguous in scale. To see why, suppose we scale \(t\) by some factor \(\lambda\). Then the essential matrix becomes:</p> \[E' = [(\lambda t)_{\leftrightarrow}] R = \lambda [t_{\leftrightarrow}] R = \lambda E\] <p>Substituting this into the epipolar constraint:</p> \[\begin{aligned} x'^T E' x &amp;= x'^T (\lambda E) x \\ &amp;= \lambda (x'^T E x) \\ &amp;= \lambda \cdot 0 \\ &amp;= 0 \end{aligned}\] <p>Since the constraint is satisfied regardless of \(\lambda\), we cannot recover the magnitude of \(t\) from the epipolar constraint alone. We can only recover the direction of \(t\), reducing our degrees of freedom from 6 to 5.</p> <h3 id="8-point-algorithm"> <a href="#8-point-algorithm" class="anchor-heading" aria-labelledby="8-point-algorithm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 8-point algorithm </h3> <p>Even though we only have 5 degrees of freedom, and therefore would typically only need 5 correspondences to solve for the essential matrix, <a href="https://www.scribd.com/document/471805325/Nister-5pt-pdf">David Nistér 2004</a> demonstrates that if you write out these constraints strictly, you end up with a system of equations that reduces to a 10th-degree polynomial that encodes the geometric (rotation and translation) relationship between these 5 degrees of freedom.</p> <p>Instead, we can use the 8-point algorithm to solve for the essential matrix without factoring in the geometric constraints, after which we can enforce the geometric constraints to ensure it’s a valid essential matrix and recover \(R\) and \(t\).</p> <p><strong>Why 8 points if \(E\) has 9 degrees of freedom?</strong> While \(E = [t_{\leftrightarrow}] R\) has only 5 true degrees of freedom (3 for rotation \(R\), 2 for the direction of translation \(t\)), we solve for it as a 3×3 matrix with 9 elements. Since we can only recover \(E\) up to a scale factor, we have 8 unknowns to solve for (9 elements - 1 scale factor = 8). Each correspondence pair \((x, x')\) gives us one linear constraint equation, so we need at least 8 correspondences to solve the system \(A \vec{e} = 0\).</p> <p>As with HW2, we use RANSAC to estimate the essential matrix by randomly sampling 8 correspondences and solving for the essential matrix expressed as a vector of variables $\vec{e}$: \(A \vec{e} = 0\)</p> <p><strong>Algorithm: 8-Point Essential Matrix Estimation</strong></p> <div style="text-align: center;"> <figure style="display: inline-block; margin: 1em 0;"> <img src="/hws/hw3/assets/ransac_8_point.png" alt="RANSAC 8-point algorithm" style="max-width: 100%;" /> <figcaption style="margin-top: 0.5em; font-style: italic;"> <strong>Algorithm 1:</strong> RANSAC 8-point algorithm to estimate essential matrix. </figcaption> </figure> </div> <p>Let’s walk through each major step of this algorithm:</p> <h3 id="step-1-point-normalization-line-3"> <a href="#step-1-point-normalization-line-3" class="anchor-heading" aria-labelledby="step-1-point-normalization-line-3"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 1: Point Normalization (Line 3) </h3> <p>Before we begin estimating the essential matrix, we normalize the image coordinates. This step is crucial for numerical stability.</p> <p>The normalization transforms 2D image coordinates \(x_i\) into normalized camera coordinates \(\hat{x}_i\) by applying the inverse camera matrix \(K^{-1}\):</p> \[\hat{x}_i \leftarrow K^{-1}[x_i; 1]\] <p>This converts from pixel coordinates to normalized image coordinates where the camera’s intrinsic parameters (focal length, principal point, etc.) are factored out. The essential matrix \(E\) relates normalized coordinates, whereas the fundamental matrix \(F\) (which we don’t use here) relates pixel coordinates directly.</p> <p><strong>Why normalize?</strong> Without normalization, the elements of the constraint matrix \(A\) (which we’ll build next) can vary by several orders of magnitude, leading to poor numerical conditioning when solving via SVD. Normalized coordinates typically have values in the range \([-1, 1]\), making the system well-conditioned.</p> <h3 id="step-2-constructing-the-constraint-matrix-lines-7-8"> <a href="#step-2-constructing-the-constraint-matrix-lines-7-8" class="anchor-heading" aria-labelledby="step-2-constructing-the-constraint-matrix-lines-7-8"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 2: Constructing the Constraint Matrix (Lines 7-8) </h3> <p>Within each RANSAC iteration, we randomly sample 8 correspondence pairs. For each pair \((\hat{x}_i, \hat{x}'_i)\), we construct a row of the constraint matrix \(A\) based on the epipolar constraint:</p> \[\hat{x}'^T_i E \hat{x}_i = 0\] <p>Writing \(\hat{x}_i = (u_i, v_i, 1)^T\) and \(\hat{x}'_i = (u'_i, v'_i, 1)^T\), we can expand this as we showed earlier. The vectorized form gives us:</p> \[\text{vec}(\hat{x}' \hat{x}^T)^T = \begin{bmatrix} u'u &amp; u'v &amp; u' &amp; v'u &amp; v'v &amp; v' &amp; u &amp; v &amp; 1 \end{bmatrix}\] <p>This becomes one row in our constraint matrix \(A \in \mathbb{R}^{8 \times 9}\). With 8 correspondence pairs, we get 8 equations (rows).</p> <h3 id="step-3-solving-via-svd-line-9"> <a href="#step-3-solving-via-svd-line-9" class="anchor-heading" aria-labelledby="step-3-solving-via-svd-line-9"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 3: Solving via SVD (Line 9) </h3> <p>We need to solve the homogeneous linear system:</p> \[A\vec{e} = 0\] <p>where \(\vec{e}\) is the 9-element vector containing the entries of \(E\) (flattened column-wise or row-wise).</p> <p><strong>Why SVD?</strong> For a homogeneous system \(A\vec{e} = 0\), the solution lies in the null space of \(A\). The Singular Value Decomposition gives us:</p> \[A = U \Sigma V^T\] <p>where the columns of \(V\) corresponding to zero (or near-zero) singular values span the null space of \(A\). Since we have 8 equations and 9 unknowns, the system is underdetermined by 1, meaning the null space is 1-dimensional. The solution \(\vec{e}\) is the last column of \(V\) (corresponding to the smallest singular value).</p> <p>Once we have \(\vec{e}\), we reshape it back into a \(3 \times 3\) matrix to get \(E_{raw}\).</p> <h3 id="step-4-enforcing-the-essential-matrix-constraint-lines-10-12"> <a href="#step-4-enforcing-the-essential-matrix-constraint-lines-10-12" class="anchor-heading" aria-labelledby="step-4-enforcing-the-essential-matrix-constraint-lines-10-12"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 4: Enforcing the Essential Matrix Constraint (Lines 10-12) </h3> <p>The matrix \(E_{raw}\) we just computed might not satisfy the properties of a true essential matrix. Recall that \(E = [t_{\leftrightarrow}] R\), where \([t_{\leftrightarrow}]\) is rank-2 and \(R\) is a rotation matrix. This structure imposes two critical constraints on \(E\):</p> <ol> <li><strong>Rank 2</strong>: \(E\) must have rank 2 (one zero singular value)</li> <li><strong>Two equal singular values</strong>: The two non-zero singular values must be equal</li> </ol> <p>To enforce these constraints, we “project” \(E_{raw}\) onto the space of valid essential matrices:</p> \[E_{raw} = U_E \Sigma V_E^T = U_E \text{diag}(\sigma_1, \sigma_2, \sigma_3) V_E^T\] <p>We then construct the corrected essential matrix by:</p> \[E = U_E \text{diag}(1, 1, 0) V_E^T\] <p>This forces the two largest singular values to be equal (set to 1) and the smallest to be exactly zero, ensuring \(E\) is rank-2 and has the proper structure.</p> <p><strong>Why does this work?</strong> This is the closest rank-2 matrix to \(E_{raw}\) in the Frobenius norm sense, and setting the two non-zero singular values to be equal enforces the constraint that comes from \(E = [t_{\leftrightarrow}] R\).</p> <h3 id="step-5-computing-inliers-line-13"> <a href="#step-5-computing-inliers-line-13" class="anchor-heading" aria-labelledby="step-5-computing-inliers-line-13"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 5: Computing Inliers (Line 13) </h3> <p>Now we evaluate how well our estimated \(E\) fits all \(N\) correspondence pairs (not just the 8 we sampled). For each normalized correspondence pair \((\hat{x}_i, \hat{x}'_i)\), we compute the <strong>epipolar distance</strong>:</p> \[d_i = (\hat{x}'_i)^T E \hat{x}_i\] <p>Ideally, if \((\hat{x}_i, \hat{x}'_i)\) are true correspondences of the same 3D point and \(E\) is correct, this distance should be zero. In practice, due to noise and measurement errors, we accept points as inliers if:</p> \[|d_i| = |(\hat{x}'_i)^T E \hat{x}_i| &lt; \tau\] <p>where \(\tau\) is a threshold (a hyperparameter you tune based on expected noise levels).</p> <p>The set of inliers:</p> \[S_{curr} = \{i \mid |(\hat{x}'_i)^T E \hat{x}_i| &lt; \tau\}\] <p>represents the correspondences that are consistent with this hypothesis for \(E\).</p> <h3 id="step-6-recovering-camera-pose-lines-19-20"> <a href="#step-6-recovering-camera-pose-lines-19-20" class="anchor-heading" aria-labelledby="step-6-recovering-camera-pose-lines-19-20"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 6: Recovering Camera Pose (Lines 19-20) </h3> <p>After RANSAC completes, we have the best essential matrix \(E_{best}\). Now we need to extract the rotation \(R\) and translation \(t\) from it.</p> <p><strong>The Ambiguity Problem</strong>: Given an essential matrix \(E\), there are <strong>four possible</strong> solutions for \((R, t)\):</p> \[\begin{aligned} &amp;(R_1, t), \quad (R_1, -t)\\ &amp;(R_2, t), \quad (R_2, -t) \end{aligned}\] <p>where \(R_1\) and \(R_2\) are two different rotation matrices and \(t\) can point in either direction.</p> <p>These four solutions can be extracted from the SVD of \(E\). The decomposition procedure (which we won’t derive here but can be found in Hartley &amp; Zisserman’s “Multiple View Geometry”) gives us these four candidates.</p> <p><strong>Cheirality Check</strong>: Only one of these four solutions is physically valid. The <strong>cheirality constraint</strong> requires that the reconstructed 3D points lie <strong>in front of both cameras</strong> (positive depth). For each of the four solutions:</p> <ol> <li>Triangulate the 3D positions of points in \(S_{best}\) using the candidate \((R, t)\)</li> <li>Check if the reconstructed 3D points have positive depth in both camera coordinate systems</li> <li>The solution where the most points (ideally all inliers) have positive depth in both cameras is the correct one</li> </ol> <p>This eliminates the ambiguity and gives us the final \((R, t)\).</p> <p><strong>Note on scale ambiguity</strong>: As discussed earlier, we can only recover \(t\) up to a scale factor. The translation vector we recover will have unit norm \(\|t\| = 1\), representing only the direction of translation. The actual scale must be determined through additional information (e.g., known object sizes or camera baselines).</p> <h1 id="step-4-triangulation-to-recover-3d-structure"> <a href="#step-4-triangulation-to-recover-3d-structure" class="anchor-heading" aria-labelledby="step-4-triangulation-to-recover-3d-structure"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 4: Triangulation to recover 3D structure </h1> <p>Now that we have recovered the camera poses (rotation \(R\) and translation \(t\) between cameras), we can use the 2D correspondences to reconstruct the 3D positions of the points in the scene. This process is called <strong>triangulation</strong>.</p> <h2 id="the-triangulation-problem"> <a href="#the-triangulation-problem" class="anchor-heading" aria-labelledby="the-triangulation-problem"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Triangulation Problem </h2> <p>Given:</p> <ul> <li>Two camera poses: Camera 1 at the origin with identity rotation, and Camera 2 with rotation \(R\) and translation \(t\)</li> <li>A correspondence pair \((\hat{x}, \hat{x}')\) in normalized image coordinates</li> <li>Camera projection matrices \(P_1\) and \(P_2\)</li> </ul> <p>We want to find the 3D point \(X\) that projects to \(\hat{x}\) in camera 1 and \(\hat{x}'\) in camera 2.</p> <p>Without loss of generality, we can set the first camera’s pose as the world coordinate frame: \(P_1 = K[I | 0] = [I | 0] \text{ (since we work in normalized coordinates)}\)</p> <p>The second camera’s projection matrix incorporates the rotation and translation: \(P_2 = [R | t]\)</p> <h2 id="the-constraint"> <a href="#the-constraint" class="anchor-heading" aria-labelledby="the-constraint"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Constraint </h2> <p>For a 3D point \(X = (X, Y, Z, 1)^T\) in homogeneous coordinates, its projection onto camera \(i\) should satisfy: \(\hat{x}_i \sim P_i X\)</p> <p>where \(\sim\) denotes equality up to scale (since we’re in homogeneous coordinates). This means that \(\hat{x}_i\) and \(P_i X\) should be parallel, or equivalently, their cross product should be zero: \(\hat{x}_i \times (P_i X) = 0\)</p> <h2 id="linear-triangulation-via-dlt"> <a href="#linear-triangulation-via-dlt" class="anchor-heading" aria-labelledby="linear-triangulation-via-dlt"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Linear Triangulation via DLT </h2> <p>The cross product constraint gives us a system of linear equations. For each camera, writing \(\hat{x} = (u, v, 1)^T\) and \(P_i = [p_i^1; p_i^2; p_i^3]\) (where \(p_i^j\) is the \(j\)-th row of \(P_i\)), the cross product expands to:</p> \[\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} \times \begin{bmatrix} p_1^T X \\ p_2^T X \\ p_3^T X \end{bmatrix} = \begin{bmatrix} v(p_3^T X) - (p_2^T X) \\ (p_1^T X) - u(p_3^T X) \\ u(p_2^T X) - v(p_1^T X) \end{bmatrix} = 0\] <p>This gives us three equations, but only two are linearly independent (the third is a linear combination of the first two). Rearranging:</p> \[\begin{aligned} u(p_3^T X) - (p_1^T X) &amp;= 0 \quad \Rightarrow \quad (u p_3^T - p_1^T) X = 0\\ v(p_3^T X) - (p_2^T X) &amp;= 0 \quad \Rightarrow \quad (v p_3^T - p_2^T) X = 0 \end{aligned}\] <p>Each camera gives us 2 equations. With 2 cameras, we get 4 equations for the 3D point \(X\) (which has 4 components in homogeneous coordinates, but only 3 degrees of freedom due to scale). Stacking these equations:</p> \[A X = \begin{bmatrix} u_1 p_{1,3}^T - p_{1,1}^T \\ v_1 p_{1,3}^T - p_{1,2}^T \\ u_2 p_{2,3}^T - p_{2,1}^T \\ v_2 p_{2,3}^T - p_{2,2}^T \end{bmatrix} X = 0\] <p>where subscripts 1 and 2 denote camera 1 and camera 2, respectively.</p> <p><strong>Solving via SVD</strong>: Just as we did for the essential matrix, we solve this homogeneous system \(AX = 0\) using SVD. The solution is the last column of \(V\) (corresponding to the smallest singular value) from the decomposition \(A = U\Sigma V^T\).</p> <p>The resulting 4D homogeneous point must be converted to 3D Euclidean coordinates by dividing by the last component: \(X_{euclidean} = \begin{bmatrix} X/W \\ Y/W \\ Z/W \end{bmatrix}\) where \(X = (X, Y, Z, W)^T\) in homogeneous coordinates.</p> <h2 id="triangulation-for-all-inliers"> <a href="#triangulation-for-all-inliers" class="anchor-heading" aria-labelledby="triangulation-for-all-inliers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Triangulation for All Inliers </h2> <p>We triangulate all correspondences in \(S_{best}\) (the inlier set from RANSAC) to reconstruct the 3D structure of the scene. Each correspondence gives us one 3D point.</p> <p><strong>Quality Check</strong>: After triangulation, it’s good practice to:</p> <ol> <li>Verify that triangulated points have positive depth in both cameras (this should be satisfied if the cheirality check was done correctly)</li> <li>Check the reprojection error: project the 3D point back onto both images and measure the distance to the original 2D points</li> <li>Discard points with large reprojection errors, as they likely correspond to outliers or poorly conditioned triangulation</li> </ol> <p><strong>Why can triangulation fail?</strong> Even with correct correspondences, triangulation can be poorly conditioned when:</p> <ul> <li>The two camera centers are too close (small baseline)</li> <li>The viewing directions are nearly parallel</li> <li>The point is very far from both cameras</li> </ul> <p>These scenarios lead to large uncertainty in the depth estimate. A good rule of thumb is that the angle between the two viewing rays should be at least 2-5 degrees for reliable triangulation.</p> <h1 id="step-5-refinement-with-bundle-adjustment"> <a href="#step-5-refinement-with-bundle-adjustment" class="anchor-heading" aria-labelledby="step-5-refinement-with-bundle-adjustment"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 5: Refinement with bundle adjustment </h1> <p>At this point, we have estimates for:</p> <ul> <li>The camera poses: \(R\) and \(t\) (from Step 3)</li> <li>The 3D structure: positions of all inlier points \(\{X_j\}\) (from Step 4)</li> </ul> <p>However, these estimates are not optimal. Why?</p> <ol> <li><strong>RANSAC gives a good but not optimal solution</strong>: RANSAC finds a robust estimate by maximizing inliers, but it doesn’t minimize reprojection error</li> <li><strong>Errors accumulate</strong>: Small errors in pose estimation affect triangulation, and vice versa</li> <li><strong>We solved components separately</strong>: The pose and structure were estimated in separate steps, not jointly</li> </ol> <p><strong>Bundle adjustment</strong> is a joint optimization that refines both camera poses and 3D structure simultaneously by minimizing the reprojection error across all observations.</p> <h2 id="the-bundle-adjustment-problem"> <a href="#the-bundle-adjustment-problem" class="anchor-heading" aria-labelledby="the-bundle-adjustment-problem"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Bundle Adjustment Problem </h2> <p>Given:</p> <ul> <li>Initial camera poses (rotations and translations)</li> <li>Initial 3D point positions</li> <li>2D observations (correspondences) of 3D points in multiple images</li> </ul> <p>We want to minimize the <strong>reprojection error</strong> - the sum of squared distances between observed 2D points and the projections of the estimated 3D points:</p> \[\min_{R, t, \{X_j\}} \sum_{i,j} \left\| x_{ij} - \pi(P_i X_j) \right\|^2\] <p>where:</p> <ul> <li>\(i\) indexes cameras (views)</li> <li>\(j\) indexes 3D points</li> <li>\(x_{ij}\) is the observed 2D position of point \(j\) in camera \(i\)</li> <li>\(P_i\) is the projection matrix for camera \(i\) (function of \(R_i, t_i\))</li> <li>\(\pi()\) is the projection function that maps 3D points to 2D</li> <li>\(X_j\) is the 3D position of point \(j\)</li> </ul> <p>The term “bundle adjustment” comes from the idea of adjusting the “bundle” of light rays (projections) from each 3D point to each camera so they all meet consistently at the 3D point positions.</p> <h2 id="why-is-this-a-non-linear-optimization"> <a href="#why-is-this-a-non-linear-optimization" class="anchor-heading" aria-labelledby="why-is-this-a-non-linear-optimization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why is this a non-linear optimization? </h2> <p>The projection function \(\pi(P_i X_j)\) involves:</p> <ol> <li><strong>Rotation</strong>: Rotating 3D points is non-linear (rotation matrices involve trigonometric functions)</li> <li><strong>Division by depth</strong>: Converting from 3D to 2D via \(\pi([X,Y,Z]^T) = [X/Z, Y/Z]^T\) introduces non-linearity</li> </ol> <p>Therefore, we cannot solve this with simple linear least squares. Instead, we use <strong>non-linear least squares optimization</strong>, typically with the <strong>Levenberg-Marquardt algorithm</strong> (a combination of gradient descent and Gauss-Newton method).</p> <h2 id="the-structure-of-bundle-adjustment"> <a href="#the-structure-of-bundle-adjustment" class="anchor-heading" aria-labelledby="the-structure-of-bundle-adjustment"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Structure of Bundle Adjustment </h2> <p>Bundle adjustment optimizes many parameters simultaneously:</p> <ul> <li>Camera parameters: For \(n\) cameras, we have rotation (3 DOF per camera) and translation (3 DOF per camera) = \(6n\) parameters</li> <li>3D point positions: For \(m\) 3D points, we have \(3m\) parameters</li> </ul> <p>Total: \(6n + 3m\) parameters to optimize!</p> <p>For a typical SfM problem with, say, 100 images and 10,000 points, that’s 30,600 parameters.</p> <p><strong>How is this tractable?</strong> The key insight is that the problem has <strong>sparse structure</strong>:</p> <ul> <li>Each 3D point is only observed in a subset of cameras</li> <li>The reprojection error for point \(j\) in camera \(i\) only depends on \(X_j\), \(R_i\), and \(t_i\)</li> </ul> <p>This means the Jacobian matrix (matrix of partial derivatives) is extremely sparse, which can be exploited by specialized sparse optimization algorithms. Libraries like <code class="language-plaintext highlighter-rouge">scipy.optimize.least_squares</code> with the <code class="language-plaintext highlighter-rouge">method='lm'</code> option or specialized SfM libraries leverage this sparsity.</p> <h2 id="implementing-bundle-adjustment"> <a href="#implementing-bundle-adjustment" class="anchor-heading" aria-labelledby="implementing-bundle-adjustment"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Implementing Bundle Adjustment </h2> <p>In practice, you would:</p> <ol> <li><strong>Parameterize the problem</strong>: <ul> <li>Represent rotations using a minimal 3-parameter representation (e.g., axis-angle or Euler angles)</li> <li>Pack all parameters into a single vector: \(\theta = [r_1, t_1, r_2, t_2, ..., X_1, X_2, ...]\)</li> </ul> </li> <li><strong>Define the residual function</strong>: <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">residuals</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">observations</span><span class="p">,</span> <span class="n">n_cameras</span><span class="p">,</span> <span class="n">n_points</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    params: packed vector of camera and point parameters
    observations: list of (camera_idx, point_idx, observed_2d_point)
    returns: vector of residuals (difference between observed and projected)
    </span><span class="sh">"""</span>
    <span class="n">camera_params</span> <span class="o">=</span> <span class="n">params</span><span class="p">[:</span><span class="n">n_cameras</span> <span class="o">*</span> <span class="mi">6</span><span class="p">]</span>  <span class="c1"># 6 per camera
</span>    <span class="n">points_3d</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">n_cameras</span> <span class="o">*</span> <span class="mi">6</span><span class="p">:].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># rest are 3D points
</span>       
    <span class="n">residuals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">cam_idx</span><span class="p">,</span> <span class="n">point_idx</span><span class="p">,</span> <span class="n">observed</span> <span class="ow">in</span> <span class="n">observations</span><span class="p">:</span>
        <span class="n">R</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="nf">extract_camera_params</span><span class="p">(</span><span class="n">camera_params</span><span class="p">,</span> <span class="n">cam_idx</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">points_3d</span><span class="p">[</span><span class="n">point_idx</span><span class="p">]</span>
        <span class="n">projected</span> <span class="o">=</span> <span class="nf">project</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="n">residuals</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">observed</span> <span class="o">-</span> <span class="n">projected</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>
</code></pre></div> </div> </li> <li><strong>Run the optimizer</strong>: <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.optimize</span> <span class="kn">import</span> <span class="n">least_squares</span>
   
<span class="n">result</span> <span class="o">=</span> <span class="nf">least_squares</span><span class="p">(</span>
    <span class="n">residuals</span><span class="p">,</span> 
    <span class="n">initial_params</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">lm</span><span class="sh">'</span><span class="p">,</span>  <span class="c1"># Levenberg-Marquardt
</span>    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">n_cameras</span><span class="p">,</span> <span class="n">n_points</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">optimized_params</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">x</span>
</code></pre></div> </div> </li> </ol> <h2 id="practical-considerations"> <a href="#practical-considerations" class="anchor-heading" aria-labelledby="practical-considerations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Practical Considerations </h2> <p><strong>Convergence</strong>: Bundle adjustment is an iterative algorithm. It typically converges in 10-50 iterations if initialized well (which is why we do RANSAC + triangulation first).</p> <p><strong>Local minima</strong>: Like all non-linear optimization, bundle adjustment can get stuck in local minima. Good initialization (from RANSAC and triangulation) is critical.</p> <p><strong>Outliers</strong>: Bundle adjustment assumes all observations are correct. A single bad correspondence can significantly corrupt the solution. It’s important to:</p> <ul> <li>Run bundle adjustment only on verified inliers from RANSAC</li> <li>Optionally use robust cost functions (e.g., Huber loss) that downweight large errors</li> </ul> <p><strong>Degeneracies</strong>: Certain camera configurations (e.g., all cameras looking in the same direction) can make the problem ill-conditioned. In practice, having diverse viewpoints improves stability.</p> <h2 id="the-impact-of-bundle-adjustment"> <a href="#the-impact-of-bundle-adjustment" class="anchor-heading" aria-labelledby="the-impact-of-bundle-adjustment"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Impact of Bundle Adjustment </h2> <p>Bundle adjustment typically reduces reprojection error by 50-90% compared to the initial RANSAC + triangulation solution. This refinement is essential for:</p> <ul> <li>High-quality 3D reconstructions</li> <li>Accurate camera pose estimation for AR/VR applications</li> <li>Multi-view stereo and dense reconstruction pipelines</li> </ul> <p>Think of bundle adjustment as the “polish” step that takes a good initial estimate and makes it great by ensuring global consistency across all cameras and points.</p> </main> <hr> <footer> <div class="d-xs-block d-md-none"> <div class="mt-4 fs-2"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </div> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
